{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Book reviews and how to fake them</h3>\n",
    "\n",
    "- Find an interesting public domain book (https://www.gutenberg.org/browse/scores/top#books-last30) and download it as plain text.\n",
    "\n",
    "\n",
    "- Use Python to massage the data into a suitable format for processing by the Latent Dirichlet Allocation (LDA) model contained in Scikit.learn. This will include removing stop words and punctuation. Some ideas for how to do this can be found <a href=\"https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\">here.</a>\n",
    "\n",
    "\n",
    "- Break the book up into small sections. The most appropriate level might vary between books, but you will most likely be breaking the book up into either paragraphs or chapters. (This might also be a pragmatic decision based on whateverâ€™s easiest.)\n",
    "\n",
    "\n",
    "- Train an LDA model on the corpus. The LDA model should find interesting topics that occur at the paragraph (or chapter) level. Be sure to explain your choice of parameters for any parameters that might have a significant effect on the model results.\n",
    "\n",
    "\n",
    "- Print out the first ten words of the ten most common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.random.dirichlet([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 99\n",
      "\n",
      "First paragraph:\n",
      " One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.\n",
      "\n",
      "Last paragraph:\n",
      " After that, the three of them left the flat together, which was something they had not done for months, and took the tram out to the open country outside the town.  They had the tram, filled with warm sunshine, all to themselves.  Leant back comfortably on their seats, they discussed their prospects and found that on closer examination they were not at all bad - until then they had never asked each other about their work but all three had jobs which were very good and held particularly good promise for the future.  The greatest improvement for the time being, of course, would be achieved quite easily by moving house; what they needed now was a flat that was smaller and cheaper than the current one which had been chosen by Gregor, one that was in a better location and, most of all, more practical.  All the time, Grete was becoming livelier.  With all the worry they had been having of late her cheeks had become pale, but, while they were talking,  Mr. and  Mrs. Samsa were struck, almost simultaneously, with the thought of how their daughter was blossoming into a well built and beautiful young lady.  They became quieter.  Just from each other's glance and almost without knowing it they agreed that it would soon be time to find a good man for her.  And, as if in confirmation of their new dreams and good intentions, as soon as they reached their destination Grete was the first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "with open('metamorphosis.txt', 'r') as myfile:\n",
    "    collection = myfile.read().split(\"\\n\\n\") #\\n\\n denotes the double linebreak used for paragraph separation\n",
    "    \n",
    "for i in range(len(collection)):\n",
    "    #replace \"\\n\" in list items with a space\n",
    "    collection[i] = collection[i].replace('\\n',' ')\n",
    "    #collection[i] = collection[i].replace(\"\\'\",'')\n",
    "    \n",
    "collection = [i.strip() for i in collection if len(i) > 1]\n",
    "\n",
    "print(f\"Number of paragraphs: {len(collection)}\\n\")\n",
    "print(f\"First paragraph:\\n {collection[0]}\\n\")\n",
    "print(f\"Last paragraph:\\n {collection[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hueyninglok/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hueyninglok/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries for cleaning and preprocessing\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning and preprocessing\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return PorterStemmer().stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def clean(doc):\n",
    "    #remove stopwords\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #remove punctuation\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #lemmatize and stem\n",
    "    lemma_stem = \" \".join(lemmatize_stemming(word) for word in punc_free.split())\n",
    "    return lemma_stem\n",
    "\n",
    "coll_clean = [clean(doc).split() for doc in collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned first paragraph:\n",
      " ['one', 'morn', 'gregor', 'samsa', 'wake', 'troubl', 'dream', 'find', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'him', 'wave', 'helplessli', 'look']\n",
      "\n",
      "Cleaned last paragraph:\n",
      " ['that', 'three', 'leav', 'flat', 'togeth', 'someth', 'do', 'month', 'take', 'tram', 'open', 'countri', 'outsid', 'town', 'tram', 'fill', 'warm', 'sunshin', 'themselv', 'lean', 'back', 'comfort', 'seat', 'discuss', 'prospect', 'find', 'closer', 'examin', 'bad', 'never', 'ask', 'work', 'three', 'job', 'good', 'hold', 'particularli', 'good', 'promis', 'futur', 'greatest', 'improv', 'time', 'be', 'cours', 'would', 'achiev', 'quit', 'easili', 'move', 'hous', 'need', 'flat', 'smaller', 'cheaper', 'current', 'one', 'choos', 'gregor', 'one', 'better', 'locat', 'and', 'all', 'practic', 'time', 'grete', 'becom', 'liveli', 'worri', 'late', 'cheek', 'becom', 'pale', 'but', 'talk', 'mr', 'mr', 'samsa', 'strike', 'almost', 'simultan', 'think', 'daughter', 'blossom', 'well', 'build', 'beauti', 'young', 'ladi', 'becom', 'quieter', 'other', 'glanc', 'almost', 'without', 'know', 'agre', 'would', 'soon', 'time', 'find', 'good', 'man', 'her', 'and', 'confirm', 'new', 'dream', 'good', 'intent', 'soon', 'reach', 'destin', 'grete', 'first', 'get', 'stretch', 'young', 'bodi']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cleaned first paragraph:\\n {coll_clean[0]}\\n\")\n",
    "print(f\"Cleaned last paragraph:\\n {coll_clean[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 1896\n",
      "\n",
      "Filtering process: Remove tokens that appear in less than 5 documents, or more than 50% of the documents.\n",
      "\n",
      "Number of words in dictionary post-filter: 451\n",
      "\n",
      "First 10 terms in filtered dictionary:\n",
      "0 abl\n",
      "1 back\n",
      "2 bed\n",
      "3 cover\n",
      "4 find\n",
      "5 hardli\n",
      "6 head\n",
      "7 him\n",
      "8 lay\n",
      "9 leg\n",
      "10 lift\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "\n",
    "#create dictionary of words in the clean collection and their integer ids\n",
    "dictionary = gensim.corpora.Dictionary(coll_clean)\n",
    "print(f\"Number of words in dictionary: {len(dictionary)}\\n\")\n",
    "\n",
    "#filter dictionary\n",
    "print(\"Filtering process: Remove tokens that appear in less than 5 documents, or more than 50% of the documents.\\n\")\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "print(f\"Number of words in dictionary post-filter: {len(dictionary)}\\n\")\n",
    "print(f\"First 10 terms in filtered dictionary:\")\n",
    "#peak at first 10 terms in dict\n",
    "i = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 10: break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (bed) appears 3 time(s).\n",
      "Word 6 (head) appears 4 time(s).\n",
      "Word 50 (get) appears 2 time(s).\n",
      "Word 63 (quit) appears 1 time(s).\n",
      "Word 72 (tri) appears 1 time(s).\n",
      "Word 73 (turn) appears 1 time(s).\n",
      "Word 78 (becom) appears 1 time(s).\n",
      "Word 79 (better) appears 1 time(s).\n",
      "Word 97 (push) appears 1 time(s).\n",
      "Word 100 (slowli) appears 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "#create word frequency dictionary for each document in the collection\n",
    "#and store in bag-of-words format\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in coll_clean]\n",
    "\n",
    "#create bag of words for paragraph 10\n",
    "bow_p10 = bow_corpus[10]\n",
    "\n",
    "#preview word frequency in paragraph 10 for first 10 words in the dictionary index.\n",
    "for i in range(10):\n",
    "    print(f\"Word {bow_p10[i][0]} ({dictionary[bow_p10[i][0]]}) appears {bow_p10[i][1]} time(s).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and training LDA model on the bow matrix.\n",
    "ldamodel = Lda(bow_corpus, num_topics=10, id2word = dictionary, passes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.017*\"right\" + 0.017*\"felt\" + 0.016*\"feel\" + 0.015*\"tri\" + 0.015*\"back\" + 0.014*\"make\" + 0.014*\"look\" + 0.013*\"part\" + 0.013*\"hard\" + 0.013*\"get\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.022*\"go\" + 0.022*\"door\" + 0.021*\"say\" + 0.018*\"thing\" + 0.017*\"it\" + 0.016*\"first\" + 0.015*\"home\" + 0.015*\"way\" + 0.014*\"father\" + 0.014*\"make\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.035*\"door\" + 0.031*\"open\" + 0.025*\"even\" + 0.024*\"famili\" + 0.018*\"one\" + 0.016*\"father\" + 0.015*\"much\" + 0.014*\"come\" + 0.014*\"sister\" + 0.014*\"never\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.023*\"long\" + 0.023*\"go\" + 0.023*\"sleep\" + 0.022*\"even\" + 0.018*\"alarm\" + 0.018*\"boss\" + 0.018*\"train\" + 0.018*\"half\" + 0.018*\"that\" + 0.018*\"see\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.025*\"father\" + 0.022*\"mother\" + 0.021*\"say\" + 0.021*\"grete\" + 0.019*\"back\" + 0.015*\"play\" + 0.015*\"gentlemen\" + 0.015*\"get\" + 0.014*\"head\" + 0.014*\"call\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.016*\"sister\" + 0.015*\"even\" + 0.014*\"go\" + 0.014*\"father\" + 0.013*\"back\" + 0.013*\"look\" + 0.013*\"come\" + 0.013*\"mr\" + 0.012*\"samsa\" + 0.012*\"door\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.033*\"mother\" + 0.027*\"sister\" + 0.022*\"father\" + 0.018*\"even\" + 0.018*\"get\" + 0.014*\"life\" + 0.013*\"becom\" + 0.013*\"perhap\" + 0.012*\"want\" + 0.011*\"take\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.019*\"even\" + 0.019*\"sister\" + 0.019*\"mother\" + 0.016*\"get\" + 0.015*\"think\" + 0.014*\"time\" + 0.013*\"it\" + 0.012*\"make\" + 0.012*\"parent\" + 0.011*\"thing\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.033*\"time\" + 0.022*\"spend\" + 0.022*\"must\" + 0.012*\"wake\" + 0.012*\"although\" + 0.012*\"forc\" + 0.012*\"there\" + 0.012*\"immobil\" + 0.012*\"show\" + 0.012*\"seem\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.026*\"clerk\" + 0.026*\"chief\" + 0.017*\"father\" + 0.016*\"back\" + 0.015*\"turn\" + 0.015*\"say\" + 0.014*\"door\" + 0.013*\"him\" + 0.013*\"make\" + 0.012*\"littl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in ldamodel.print_topics():\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
