
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{CS156 Final Project: Predicting Fanart Popularity using Artist Attributes and
Image Features}
    \author{Huey Ning Lok}
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
\section{Problem Statement}

Predicting image popularity is an interesting and potentially profitable
task considering that many users post images on social media platforms
to increase social and commercial influence - the more popular the
image, the greater the influence (De Veirman, Cauberghe, \& Hudders, 2017). Analyzing image popularity also provides insight into the mindset and proclivity of the audience, and prompts us to consider the various
factors involved in promoting image popularity.\\

In this project, I aim to use machine learning techniques to predict the
popularity of artwork scraped from DeviantArt.com - specifically fanart
for the Sherlock Holmes fandom. Both artist attributes and the images
themselves will be considered when building the predictive model(s).

\subsection{Deviantart}

Deviantart.com is "the world's largest online social community for
artists and art enthusiasts" (Deviantart, 2019). The website was
launched on August 7, 2000, and is still up and running with an active
community of artists and viewers alike. According to their About page,
they have over 44 million registered members and attract over 45 million
unique visitors per month. Members upload tens of thousands of original
pieces of artwork every day (Deviantart, 2019).\\

I chose to scrape Deviantart for the data since a large portion of the
user-submitted images consist of fanart.

\subsection{Fanart}

Fan art is artwork created by fans of a work of fiction and derived from
a series character or other aspect of that work (Wikipedia contributors,
2019). It is an interesting subcategory of art to explore since it is
explicitly derived from a specific fictional work - which has its own
artistic style - and seeks to replicate, or at least pay tribute to some
elements of the original work.
\newpage
\subsection{Sherlock Holmes}

Sherlock Holmes is a fictional private detective created by British
author Sir Arthur Conan Doyle(Wikipedia contributors, 2019) According to the
Guinness World Records, Sherlock Holmes has the "world record for the
most portrayed literary human character in film \& TV", having been
depicted on screen 254 times (Guinness World Record, 2012). The most
recent and arguably most popular adaptations include the 2009 movie
adaptation staring Robert Downey Jr. and the 2010 - 2017 BBC One series
staring Benedict Cumberbatch.\\

The graph below depicts the worldwide relative frequency of a Google
search for the "Sherlock Holmes" keyword over time (Google Trends, 2019). The popularity of
the term peaks at the tail end of 2009 and 2011, which aligns with the
release dates of the movie and tv series.\\

I chose Sherlock Holmes as the target fandom on the basis that it had a
large fanbase.\\

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}220}]:} \PY{n}{google\PYZus{}trend\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sherlock\PYZhy{}holmes\PYZhy{}trend.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{skiprows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{40}\PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{250}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Google Trends \PYZhy{} Worldwide Interest in }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Sherlock Holmes}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ Over Time (2004 \PYZhy{}}
          \PY{l+m+mi}{2019}\PY{p}{)}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{, size=40)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{fontsize}\PY{o}{=}\PY{l+m+mi}{13}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time (Month and Year)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Relative Frequency}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{google\PYZus{}trend\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{google\PYZus{}trend\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}220}]:} [<matplotlib.lines.Line2D at 0x24c6eb9320>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.5\paperheight}}{DA-analysis_files/DA-analysis_1_1.png}
    \end{center}

    \newpage
    \section{Solution Specification}

\subsection{Features}

\subsubsection{Predictors}

Prior research on predicting image popularity showed that leveraging
both image features as well as social cues allowed the reliable
prediction of "the normalized view count of images with a rank
correlation of up to 0.81" (Khosla, Das Sarma, \& Hamid, 2014).\\

Intuitively, we are aware that image popularity cannot be predicated on
solely social cues or image aesthetics. It is trivial to see that the
more popular the content-creator, the more popular their content will
be. As for image aesthetics, Dufour (2014) makes a neat statement in his
paper on image popularity prediction: ``it's clear, at least trivially,
that {[}image popularity{]} cannot be due entirely to exogenous features
(i.e., features that are not contingent on the image itself). As proof,
we offer the fact that the majority of imagespace is images of noise,
yet very few images of noise are widely popular. Furthermore, images
that are objectively aesthetic or attractive---either due to composition
or content---are plainly more likely to be popular.''\\

The predictors will thus consist of social cues in the form of artist
attributes, and image features extracted via deep learning. A detailed
breakdown of the extracted features is shown in the Data Importing
section.

\subsubsection{Response variable}

While there are various metrics to consider when judging image
popularity (number of views, comments, favorites, downloads, etc.), I
will simply be using number of image favorites as a proxy for
popularity. In their study, Khosla et al. used number of image views as a proxy (2014). In future work, one could consider how a mixture of various metrics may lead to the "best" popularity measure.

\subsection{Models}

\subsubsection{Artist Attributes}

Four linear regression models are trained as well as a neural network
model to compare their performances.\footnote{\textbf{\#regression:} While sklearn's linear regression library makes using various linear regression models a trivial task, it is still important to have an understanding of the differences between each linear regression method used. In my explanations, I detail these differences, suggest using cross-validation for determining penalty terms, and also compare linear regression models to using a neural network model.} The models and their explanations are shown below:

\subsubsection*{Ordinary Least Squares (OLS) Regression}

OLS regression determines parameter values by minimizing the sum of the
squared residuals. In a 2D space, the regression can be seen as finding
the best-fit line between the datapoints. In higher-dimensional spaces,
the best-fit hyperplane is found through minimizing squared residuals.
Since there is no bias introduced through a penalty parameter, OLS
regression models can have a large amount of variance.

\subsubsection*{Ridge Regression}

Ridge regression is only known as Penalized Least Squares (PLS)
regression. It introduces a small amount of bias into how the line is
fit to the data to significantly drop the variance of the model and
minimize overfitting. Ridge regression determines parameter values by
minimizing (sum of squared residuals) + (λ x the slope\^{}2) where λ is
the regularization parameter and can be determined via cross-validation
using sklearn's \texttt{RidgeCV()} function.

\subsubsection*{Lasso Regression}

Similar to ridge regresssion, lasso regression introduces bias into the
training procedure by minimizing (sum of squared residuals) + ( λ x the
slope ). As seen from the minimizing equation, ridge regression can only
shrink the slope asymptotically close to 0, while lasso regression can
shrink the slope all the way to 0. As such, lasso regression is superior
to ridge regression at reducing variance in models that contain a lot of
useless variables. λ can be determined via cross-validation using
sklearn's \texttt{LassoCV()} function.

\subsubsection*{Elastic Net Regression}

Elastic net regression minimizes the sum of squared residuals + lasso
regression penalty + ridge regression penalty. Elastic net regression
combines elements of both ridge and lasso by grouping and shrinking the
parameters associated with correlated variables, and leaving them in the
equation or removing them all at once. The λ's for both the ridge and
lasso penalty components can be determined via cross-validation using
sklearn's \texttt{ElasticNetCV()} function.

\subsubsection*{Neural Network Model}

While the regression models listed above all assume underlying linear
relationships within the data, a neural network with a single hidden
layer with nonlinear activation functions is considered to be a
Universal Function Approximator, i.e. capable of learning any function.
Since neural networks are capable of learning highly complex functions,
they should perform better if there are many non-linear relationships
within the data. Due to the high number of parameters, neural networks
can be prone to overfitting, and so a 0.2 cross-validation split is
specified for continuous validation throughout the training process.

\subsubsection{Image Features}

\subsubsection*{Convolutional Neural Network (CNN)}

A convolutional neural network (CNN) is trained to predict image
popularity based on image features.\\

CNNs are appropriate for processing images vs conventional
fully-connected neural networks since it enforces a sparse local
connectivity pattern between neurons of adjacent layers, i.e. each
neuron is connected to only a small region of the input volume
(Karpathy, 2016). This takes advantage of the spatial information
embedded within the image, and also reduces the number of parameters
needed at each layer, allowing deeper networks to be constructed with
fewer parameters (Karpathy, 2016). In short, CNNs take advantage of the spatial relationships between an
image's pixels to deliver more efficient and more accurate performance.

\subsubsection*{Transfer Learning}

The CNN will be an adaptation of the VGG16 model designed by Simonyan
and Zisserman (2014). Distinctive features of the VGG16 model include
very deep convolutional layers (up to 19 weight layers) and small
convolution filters (3 x 3), which allowed the model to achieve 92.7\%
top-5 test accuracy in ImageNet Large Scale Visual Recognition
Competition (ILSVRC), thus allowing the authors to conclude that
"representation depth is beneficial for the classification accuracy"
(Simonyan \& Zisserman, 2014).\\

The model also generalises well to a wide range of tasks and datasets.
The authors tested the model on a variety of classification tasks
outside of ImageNet (which mostly consisted of 10K + images with labels)
and found consistent average precision (AP) scores of
\textasciitilde{}89\%.\\

Since Khosla et al. (2014) built on Krizhevsky, Sutskever, and Hinton's (2012)
CNN model (which has about 1 - 5 weight layers) to predict image
popularity, and VGG16 has outperformed their original CNN model in
various ILSVRC competitions, it would appear to be a good choice to
adapt the VGG16 model for my own use in predicting image popularity.\\

The downside of using VGG16 is that it takes a long time to train
(Hassan, 2018), which makes sense considering the deep convolutional
layer architecture with roughly 14 million trainable parameters. To
reduce the training time, I implement transfer learning by freezing the
VGG16 model with 'ImageNet' weights - the weights which were used to
classify ImageNet images - removing the top fully-connected layer and
subtituting it with my own, and only training the weights in my newly
added layers. This reduces the trainable parameters to roughly 2.5
million instead. Due to time constraints, I will also only be training
the model for 50 epochs with a batch size of 30 images.\\

It should be noted that the original model was trained for image
classification, while I am using it for a regression problem. While a
new output layer with a linear activation function has been added to
make continuous predictions, the model's main architecture was still
designed with a classification task in mind, as opposed to predicting
the value of some metric (in this case, the number of favorites an image
will receive). Furthermore, the model was meant to be used on real-life
images, while I am using it on fanart, which can be life-like or
cartoonish in nature depending on the artist's style.\\

Since the CNN mostly operates as a black-box, i.e. the predictive power
of the image features as calculated by the CNN are hard to interpret. In
future work, I could implement techniques of extracting high-level image
featues as well such as color intensity, color hue, color contrast, etc.
and using them as inputs in more interpretable algorithms such as linear
regression, support vector machines, etc. as performed by Khosla et al.
(2014).\footnote{\textbf{\#decisionselection:} I elaborated on the decision framework that I followed to decide on using a CNN for predicting image popularity based on image features. Various factors were considered such as the CNN's architectural suitability (which makes the training process both more efficient and accurate), the impressive results from previous deep CNNs built for image classification, the usage of CNNs in a previous task similar to mine (Khosla et al., 2014), and the techniques of transfer-learning and layer-freezing that can be used to save on computational time.}
\newpage
    \section{Data Collection}

To gather the data, I scraped Deviantart for images categorized under
the 'sherlock' search query. The exact url used was:
\url{https://www.deviantart.com/popular-all-time/?q=sherlock&offset=0}. The
images were supposedly ordered according to "popular of all time", but
I found a mix of images with both high favorite and low favorite counts
within my sample size of 5638 images out of the \textasciitilde{}190,000
results available. Alongside the images themselves, social metrics in
the form of views, downloads, favorites, and various artist attributes
were also scraped from the image and artist pages.\\

The web-scraper was built using a free and open-source Python
web-scraping framework, scrapy. I scraped only image posts, and made
sure to avoid scraping posts of other formats, e.g. literature. I set a
download delay of 250 ms - which had inbuilt scrapy randomization
applied to it - as a form of web-scraping etiquette by not downloading a
bulk of images continuously, and also to avoid my web-scraper's behavior
being obviously detectable as non-human (just in case).

\subsection{Number of images downloaded}

The reiteration process to fix bugs took time, and my scraper also
crashed even on a few bug-free runs, so I settled on downloading 5638
images for now. The scraped images were stored in my local drive under
the folder, 'DA-images-2'. The image data was stored in a json file,
'image-data-2'.\\

A full breakdown of the scraped data can be found below in the Data
Importing section. The web-scraper code can be found in the Appendix \ref{appendix:scraper}.\footnote{\textbf{\#algorithms:} I learned how to use the python web-scraping package, scrapy, to write my own web-scraper for scraping data from DeviantArt. As such, I demonstrated a competence in reading documentation, self-learning, and exploration in order to write an algorithm to automate the scraping process so that data can be collected in high-volume in an efficient and convenient manner for this machine learning project.}
\newpage
       \section{Data Importing}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{}import libraries}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{display.max\PYZus{}colwidth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}for displaying the full image urls so that}
        \PY{n}{they} \PY{n}{are} \PY{n}{clickable} \PY{p}{(}\PY{k}{if} \PY{n}{needed}\PY{p}{)}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.25}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}json}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZhy{}data\PYZhy{}2.json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}              account\_age             artist\_asl artist\_comments  \textbackslash{}
        0  Deviant for 5  Years   Female/France          468              
        1  Deviant for 8  Years   Female/Russia          169              
        2  Deviant for 6  Years   Female/Japan           23               
        3  Deviant for 13  Years  Female/France          322              
        4  Deviant for 7  Years   Female/South Korea     436              
        5  Deviant for 10  Years  United Kingdom         17               
        6  Deviant for 7  Years   28/Female/Russia       8,264            
        7  Deviant for 12  Years  France                 376              
        8  Deviant for 6  Years   Female/United Kingdom  128              
        9  Deviant for 7  Years   Female/United States   32               
        
           artist\_critiques artist\_deviations      artist\_dob artist\_faves  \textbackslash{}
        0  0.0               183               March 5         1,830         
        1  0.0               36                March 5         216           
        2  0.0               28                September 9     53            
        3  0.0               55                March 29        396           
        4  0.0               73                September 17    37            
        5  0.0               77                NaN             6             
        6  0.0               150               April 21, 1990  3,760         
        7  0.0               13                NaN             114           
        8  0.0               97                NaN             0             
        9  0.0               8                 March 27        30            
        
          artist\_forum\_posts artist\_page\_views artist\_scraps   {\ldots}    comments  \textbackslash{}
        0  0                  10,634            0              {\ldots}     22.0      
        1  0                  11,377            2              {\ldots}     45.0      
        2  0                  9,907             0              {\ldots}     102.0     
        3  0                  36,431            3              {\ldots}     35.0      
        4  0                  45,119            0              {\ldots}     8.0       
        5  0                  18,474            0              {\ldots}     21.0      
        6  0                  68,208            0              {\ldots}     46.0      
        7  0                  6,566             0              {\ldots}     5.0       
        8  0                  29,442            0              {\ldots}     22.0      
        9  0                  2,958             2              {\ldots}     38.0      
        
                 date\_posted downloads   faves hashtags  \textbackslash{}
        0  January 17, 2014   92.0      142.0   NaN       
        1  December 30, 2011  186.0     1135.0  NaN       
        2  August 11, 2012    225.0     1959.0  NaN       
        3  January 21, 2012   48.0      551.0   NaN       
        4  August 19, 2011    34.0      91.0    NaN       
        5  August 1, 2010     59.0      696.0   NaN       
        6  February 20, 2012  46.0      218.0   NaN       
        7  January 27, 2012   14.0      96.0    NaN       
        8  July 13, 2013      8.0       66.0    NaN       
        9  December 26, 2011  39.0      877.0   NaN       
        
                                                                                                  image\_links  \textbackslash{}
        0  https://www.deviantart.com/get-sherlock/art/Sherlock-427568754                                       
        1  https://www.deviantart.com/masterhalfling/art/Sherlock-276657827                                     
        2  https://www.deviantart.com/donperico/art/SHERLOCK-320465780                                          
        3  https://www.deviantart.com/cheeky-bee/art/Sherlock-Of-the-Dead-280821239                             
        4  https://www.deviantart.com/hahaaaaaaaaaaaa/art/Sherlock-253839329                                    
        5  https://www.deviantart.com/1stclassstamps/art/Sherlock-Holmes-173600158                              
        6  https://www.deviantart.com/feyjane/art/Sherlock-286116362                                            
        7  https://www.deviantart.com/elsias/art/Sherlock-281791771                                             
        8  https://www.deviantart.com/sherlockthegame/art/Sherlock-The-Game-Is-On-Character-Contest-385061745   
        9  https://www.deviantart.com/fractionofadot/art/Sherlock-276038401                                     
        
                                                   image\_paths  \textbackslash{}
        0  [full/c11741fda71c47368b3ecc42de439de62c48cf30.jpg]   
        1  [full/09fbbe05751a1b08cf03445e0821ca44dd5b547b.jpg]   
        2  [full/0f13dd8004eb7f0ce2b45c826f4e76df5978ac13.jpg]   
        3  [full/2d3083ae52b6ea7b69fdd83c0f74c5fd8525a489.jpg]   
        4  [full/be06d405f58195ef9d4c5c42c1c6cb2edb673754.jpg]   
        5  [full/11dc8450503f97976fba168972410295b76c72ed.jpg]   
        6  [full/587559fae93b85cbfa8802d39e7a2e1d3cbe415c.jpg]   
        7  [full/9507a844991ab37afec9bc0a0f619bfa0d033b04.jpg]   
        8  [full/45901b4a9092329595f2e6739d213b2d169e45f9.jpg]   
        9  [full/c752fa217f32f5510a49c4f17f3980db9a442c68.jpg]   
        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     image\_urls  \textbackslash{}
        0  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/e9437807-b7f1-4c5b-a26b-f9e648379922/d72ka5u-e95f0672-311e-444d-91d1-fa2965b01f90.png/v1/fill/w\_1024,h\_537,q\_80,strp/sherlock\_by\_get\_sherlock\_d72ka5u-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NTM3IiwicGF0aCI6IlwvZlwvZTk0Mzc4MDctYjdmMS00YzViLWEyNmItZjllNjQ4Mzc5OTIyXC9kNzJrYTV1LWU5NWYwNjcyLTMxMWUtNDQ0ZC05MWQxLWZhMjk2NWIwMWY5MC5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.y4LtbGzQLxiWRZVSYC3OuJMVbv6XfvgSIGkGIzv47XU]                                     
        1  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/dee153a3-1985-4a20-9c5a-8728f6aaf611/d4kpqjn-052b7481-dbd7-4e8a-bee2-465338941689.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2RlZTE1M2EzLTE5ODUtNGEyMC05YzVhLTg3MjhmNmFhZjYxMVwvZDRrcHFqbi0wNTJiNzQ4MS1kYmQ3LTRlOGEtYmVlMi00NjUzMzg5NDE2ODkuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.MGdgMPuuE2eZiv6yrT-gZ1d8CoLLdcNFLkxwOAnZs2A]                                                                                                                                                                   
        2  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c0acf0b0-e7e6-4c7c-8ff6-7763aeb3dd79/d5asoz8-ec94075b-c573-45a5-8bb0-d52e49232e96.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2MwYWNmMGIwLWU3ZTYtNGM3Yy04ZmY2LTc3NjNhZWIzZGQ3OVwvZDVhc296OC1lYzk0MDc1Yi1jNTczLTQ1YTUtOGJiMC1kNTJlNDkyMzJlOTYuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.raEwrdO3gYAzPWV0BaonYhXWzcC3pAm-VMMFxdhLLec]                                                                                                                                                                   
        3  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/bacca111-4256-4446-90e2-4aec411ca445/d4n6z1z-09894a11-50e9-4767-8804-f6dd4bc9c269.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2JhY2NhMTExLTQyNTYtNDQ0Ni05MGUyLTRhZWM0MTFjYTQ0NVwvZDRuNnoxei0wOTg5NGExMS01MGU5LTQ3NjctODgwNC1mNmRkNGJjOWMyNjkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.fKxTuklGbcGlYbIM0S\_AAkRVp9MDet\_yMD-42N8WWvg]                                                                                                                                                                   
        4  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/75a9b8f3-6cde-4380-a50b-53a616ff3cbe/d474noh-8b31aa72-9e42-4006-afc0-1653023048d9.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzc1YTliOGYzLTZjZGUtNDM4MC1hNTBiLTUzYTYxNmZmM2NiZVwvZDQ3NG5vaC04YjMxYWE3Mi05ZTQyLTQwMDYtYWZjMC0xNjUzMDIzMDQ4ZDkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.MN8jGrrVbhVG6B5zmQsmyWaGcxPYSTDA7xIItuUn8Ig]                                                                                                                                                                   
        5  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0725e00d-6f41-4f48-899c-5c8ed95814fe/d2vcuqm-25cee738-e834-4187-bd77-22a9dd88d8e3.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzA3MjVlMDBkLTZmNDEtNGY0OC04OTljLTVjOGVkOTU4MTRmZVwvZDJ2Y3VxbS0yNWNlZTczOC1lODM0LTQxODctYmQ3Ny0yMmE5ZGQ4OGQ4ZTMucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.OtN6CRHb7viL9pMiMrg0GvW4PCf3WxIyFp\_6gmhxHx4]                                                                                                                                                                   
        6  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/4da4813c-23ac-4c24-a4af-948eaa298496/d4qcgsq-56275fbc-a0d4-4045-9a80-f77a2fb2c981.jpg/v1/fill/w\_900,h\_632,q\_75,strp/sherlock\_by\_feyjane\_d4qcgsq-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NjMyIiwicGF0aCI6IlwvZlwvNGRhNDgxM2MtMjNhYy00YzI0LWE0YWYtOTQ4ZWFhMjk4NDk2XC9kNHFjZ3NxLTU2Mjc1ZmJjLWEwZDQtNDA0NS05YTgwLWY3N2EyZmIyYzk4MS5qcGciLCJ3aWR0aCI6Ijw9OTAwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.NKBehw9Lh1lYIHt8tTQ\_jG08rgbSBEZ-WOskbTKxi60]                                            
        7  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/06502dcd-0aae-4239-847b-a3355b8342d0/d4nrrx7-ec385512-9bea-46aa-8eed-3bb99981cc5d.jpg/v1/fill/w\_600,h\_770,q\_75,strp/sherlock\_by\_elsias\_d4nrrx7-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NzcwIiwicGF0aCI6IlwvZlwvMDY1MDJkY2QtMGFhZS00MjM5LTg0N2ItYTMzNTViODM0MmQwXC9kNG5ycng3LWVjMzg1NTEyLTliZWEtNDZhYS04ZWVkLTNiYjk5OTgxY2M1ZC5qcGciLCJ3aWR0aCI6Ijw9NjAwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.MFrdMPlD0MCVREsbhl0-bc6r4QQTcx1jW47a\_e3hnq8]                                             
        8  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/bff32934-e726-4d2c-85f9-2ed211cc4e46/d6d97jl-7b5d62ec-1167-49a3-aff4-1da141868ab6.png/v1/fill/w\_1024,h\_750,strp/sherlock\_\_the\_game\_is\_on\_\_character\_contest\_\_by\_sherlockthegame\_d6d97jl-fullview.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NzUwIiwicGF0aCI6IlwvZlwvYmZmMzI5MzQtZTcyNi00ZDJjLTg1ZjktMmVkMjExY2M0ZTQ2XC9kNmQ5N2psLTdiNWQ2MmVjLTExNjctNDlhMy1hZmY0LTFkYTE0MTg2OGFiNi5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.8FJFQbkoZhTxoISgGnPk7nl7lDh3lTbB5JU0qlZSD5c]   
        9  [https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c5b2ce3d-fee1-43ef-b7cd-6c8fea631e79/d4kcgld-a37fe0a1-67da-437f-9065-924a4776a849.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2M1YjJjZTNkLWZlZTEtNDNlZi1iN2NkLTZjOGZlYTYzMWU3OVwvZDRrY2dsZC1hMzdmZTBhMS02N2RhLTQzN2YtOTA2NS05MjRhNDc3NmE4NDkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.eqBpfLqmpgdZvMgTw11YDY6ul6pJZmsFWbydy-WOZI8]                                                                                                                                                                   
        
                                                 titles    views  
        0  Sherlock                                      1606.0   
        1  Sherlock                                      14153.0  
        2  SHERLOCK                                      21163.0  
        3  Sherlock Of the Dead                          6988.0   
        4  Sherlock                                      2742.0   
        5  Sherlock Holmes                               14566.0  
        6  Sherlock                                      3139.0   
        7  Sherlock                                      2208.0   
        8  Sherlock: The Game Is On (Character Contest)  2983.0   
        9  Sherlock                                      10189.0  
        
        [10 rows x 23 columns]
\end{Verbatim}
            
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5638 entries, 0 to 5637
Data columns (total 23 columns):
account\_age           5621 non-null object
artist\_asl            5621 non-null object
artist\_comments       5621 non-null object
artist\_critiques      5621 non-null float64
artist\_deviations     5621 non-null object
artist\_dob            4373 non-null object
artist\_faves          5621 non-null object
artist\_forum\_posts    5621 non-null object
artist\_page\_views     5621 non-null object
artist\_scraps         5621 non-null object
artist\_urls           5621 non-null object
artist\_watchers       5621 non-null object
artists               5638 non-null object
comments              5629 non-null float64
date\_posted           5638 non-null object
downloads             4088 non-null float64
faves                 5636 non-null float64
hashtags              653 non-null object
image\_links           5638 non-null object
image\_paths           5638 non-null object
image\_urls            5638 non-null object
titles                5638 non-null object
views                 5636 non-null float64
dtypes: float64(5), object(18)
memory usage: 1013.2+ KB

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.0\paperheight}}{DA-analysis_files/artist-col.png}
    \end{center}

            
    \section{Data Cleaning and Preprocessing}

The dataframe info reveals that some of the columns are not in the right
format, and there are also some missing values. Most - if not all - of
the missing values can be rationally explained, as shown in the table
above. While I would ideally have cleaned all the columns, some of the
columns required non-trivial cleaning methods, and so they were omitted
due to time constraints. Given additional time and resources, I believe
that the "ignored" columns should be cleaned as they could potentially
provide significant predictive power to the model, bearing in mind that
those with a significant number of missing values may skew the model if
an appropriate filtering method is not chosen.\\

The following section details the cleaning process of all the "cleaned"
columns, and the section below this lists the "ignored" columns as well
as brief explanations as to why their cleaning process would be
non-trivial. In some cases, the portion of non-null objects is too small
to justify cleaning the column, e.g. hashtags only has 653 data points
out of 5638 within the entire dataset.\footnote{\textbf{\#decisionselection:} The data cleaning and preprocessing step requires the heavy use of appropriate heuristics and frameworks for making decisions. For example, I had to decide on which columns I would keep, would clean, would ignore, etc. I had to weigh the costs and benefits of cleaning columns based on the difficulty of the process, the significance of the column, the number of data points, etc. Each choice I made was carefully reasoned and justified so that my decision path is transparent and understandable, both for myself and the audience.}

\subsection{Cleaned Columns}

Cleaned columns are columns that were included in the cleaning process.
This means that the columns fit the following criteria:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  need to be cleaned due to improper format;
\item
  have strong justification for being cleaned (sufficient data points,
  cleaning process does not take too long, etc.).
\end{enumerate}

Do note that columns which need not be cleaned may still be included in
the final dataframe to be analyzed, even if they are not listed. Cleaned columns: account\_age, artist\_comments, artist\_deviations, artist\_faves, artist\_forum\_posts, artist\_page\_views, artist\_scraps, artist\_watchers, date\_posted.

   \subsubsection{Account Age}

The account age column is cleaned from a string format of "Deviant for
-\/- Years/Months" to be the time delta of the account age in days.
There are 17 NaN values leftover after the cleaning process since there
are 17 banned artists, aka artists whose accounts have been disabled,
and so do not have a valid artist\_url to extract data from.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{}cleaning account\PYZus{}age column}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original calue counts in account\PYZus{}age column}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{account\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}use test\PYZus{}df first since cleaning process is a bit tricky \PYZhy{} avoid overwriting original}
        \PY{n}{df}
        \PY{n}{test\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{aa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{account\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZca{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{s*\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{,} \PY{n}{regex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{}replace empty}
        \PY{n}{spaces} \PY{k}{with} \PY{n}{nan}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{aa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{extract}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{d+)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)} \PY{c+c1}{\PYZsh{}extract time value}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}unit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{aa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{6}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{}extract time unit}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}unit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}unit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}strip time unit of any blank}
        \PY{n}{spaces}
        
        \PY{c+c1}{\PYZsh{}if time unit is months, convert value to month time delta, else convert to year time}
        \PY{n}{delta}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time\PYZus{}unit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Months}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                    \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}timedelta}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{unit}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
                                     \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}timedelta}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{unit}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cleaned value counts}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{n}{dropna}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}lastly, assign cleaned column back to original df}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{account\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}
\newpage
    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Original value counts in account\_age column
Deviant for 7  Years      873
Deviant for 8  Years      830
Deviant for 9  Years      691
Deviant for 10  Years     617
Deviant for 11  Years     570
Deviant for 6  Years      465
Deviant for 12  Years     412
Deviant for 5  Years      281
Deviant for 13  Years     256
Deviant for 14  Years     192
Deviant for 15  Years     157
Deviant for 4  Years      111
Deviant for 3  Years      53
Deviant for 16  Years     50
Deviant for 2  Years      39
NaN                       17
Deviant for 1  Year       13
Deviant for 17  Years     6
Deviant for 18  Years     1
Deviant for 7  Months     1
Deviant for 9  Months     1
Deviant for 10  Months    1
Deviant for 5  Months     1
Name: account\_age, dtype: int64

Cleaned value counts
2556 days 16:44:24    873
2921 days 22:33:36    830
3287 days 04:22:48    691
3652 days 10:12:00    617
4017 days 16:01:12    570
2191 days 10:55:12    465
4382 days 21:50:24    412
1826 days 05:06:00    281
4748 days 03:39:36    256
5113 days 09:28:48    192
5478 days 15:18:00    157
1460 days 23:16:48    111
1095 days 17:27:36    53
5843 days 21:07:12    50
730 days 11:38:24     39
NaT                   17
365 days 05:49:12     13
6209 days 02:56:24    6
6574 days 08:45:36    1
213 days 01:23:42     1
304 days 08:51:00     1
273 days 22:21:54     1
152 days 04:25:30     1
Name: value, dtype: int64

    \end{Verbatim}
\newpage
   \subsubsection{ Date posted}

The date posted column is cleaned by converting it from string to a
datetime object. A new column, "image\_age" (whose unit is in days) is
created by taking the difference between the date of the latest image
posted in the dataset and the image's own posted date. This means that
older images will have larger "image\_age" values, while newer images
will have smaller "image\_age" values, with the newest image having an
"image\_age" of 0.\\

The "image\_age" column has 14 NaN values since some of the dates posted
do not contain years, and so an error is produced when trying to
calculate the image age. These errors are coerced to a Nan format.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{}cleaning date\PYZus{}posted column by converting dates to datetime}
        \PY{c+c1}{\PYZsh{}a few dates don\PYZsq{}t have years, which makes their \PYZdq{}age\PYZdq{} quite meaningless.}
        \PY{c+c1}{\PYZsh{}The errors are coerced so produce NaN outputs, which will later be removed from the}
        \PY{n}{dataset}\PY{o}{.}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{errors}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coerce}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}get number of days between image post date and newest post date in dataset}
        \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{days}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Newest date in dataset: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{test\PYZus{}df[}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{date\PYZus{}posted}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{].max()\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of Nans: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{sum(test\PYZus{}df[}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{image\PYZus{}age}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{].isna())\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}assign image\PYZus{}age and date\PYZus{}posted column to actual df}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Newest date in dataset: 2018-12-30 00:00:00
Number of Nans: 14

    \end{Verbatim}

    \subsubsection{Artist attributes, Image paths and urls}

The artist attribute columns and image paths and url columns have a
rather trivial cleaning process. For the artist attribute columns, I
just removed all commas and converted the strings to floats. The
image\_paths and image\_urls columns consisted of a single element within
a list, so I just extracted the element for easier processing later on.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{}cleaning artist attributes \PYZhy{} done in bulk because trivial}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}deviations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}deviations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}forum\PYZus{}posts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}forum\PYZus{}posts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}page\PYZus{}views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}page\PYZus{}views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}scraps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}scraps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}watchers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}watchers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{float}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Artist attributes cleaned.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}clean image\PYZus{}paths and image\PYZus{}urls by extracting the string from list}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{str}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Image path and url cleaned.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Artist attributes cleaned.
Image path and url cleaned.

    \end{Verbatim}
\newpage
   \subsection{Ignored columns}

Ignored columns are columns that were omitted from the cleaning process
despite needing to be cleaned, usually because the cleaning process is
too time-consuming (or beyond my skill-level), or the column has too few
data points. There are some columns that will be ultimately ignored in
the final process, but are not listed here since the data was in the
right format (aka they do not need to be cleaned).

\subsubsection*{'artist\_asl'}

Data type: Array of the age, gender, and country of the artist.\\

This is a non-trivial cleaning process since artists may choose to list
none to all of the categories listed above, and so trying to parse the
list order and assign elements to the proper categories of age, gender,
and country can be challenging. Some of the values are also 'Unknown'.

\subsubsection*{'artist\_dob'}

Data type: A string of the artist's date of birth.\\

The column was ignored for two reasons. First, there are only 2678
non-null entries. Second, a good amount of entries do not contain a year
within the birthdate, which one would assume to have the most potential
significant influence.

\subsubsection*{'hashtags'}

Data type: Array of hashtags posted with the image entry.\\

The column has only 653 non-null entries, and so was omitted. I also
realized later on that I did not scrape the hashtag field properly, and
so when the hashtags exceed a certain number, the "(show more)" string
is extracted instead of the additional hidden hashtags.\\

In future work, it would be interesting to use the number of hashtags as
a potential feature, or use natural language processing techniques to
identify patterns in the hashtags.

\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}dropna to demonstrate the ignore columns data formats}
        \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}asl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}dob}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:}                             artist\_asl          artist\_dob  \textbackslash{}
        25   30/Female/Russia                   February 26, 1989    
        51   Female/People's Republic of China  June 3               
        61   Male/United Kingdom                August 10            
        64   Other/Spain                        May 8                
        77   Female/Spain                       November 11          
        85   Male/Sweden                        June 26              
        87   France                             July 19              
        90   Unknown                            October 23           
        97   Female/United States               November 18          
        101  19/Female/South Korea              September 11, 1999   
        
                                                                                                                                                   hashtags  
        25   [tv, thelastproblem, art, amazing, artdigital, bbc, beautiful, blackandwhite, (show more)]                                                      
        51   [sherlock, sherlockholmes, johnlock, bbcsherlock]                                                                                               
        61   [portrait, sherlock, sherlockholmes, benedictcumberbatch, fanarttraditionalart]                                                                 
        64   [bbc, benedict, portrait, sherlock, sweet, sherlockbbc, benedictcumberbach, sherlocktvshow, (show more)]                                        
        77   [sherlock, sherlockholmes, watson, martinfreeman, benedictcumberbatch, sherlockbbc, sherlockholmesbenedictcumberbatch, watsonbbc, (show more)]  
        85   [bbc, design, holmes, max, sherlock, statue, grecke]                                                                                            
        87   [fanart, sherlock, sherlockholmes, benedictcumberbatch]                                                                                         
        90   [sherlock, sherlockholmes, benedictcumberbatch, sherlockbbc, sherlockfanart, benedict\_cumberbatch]                                              
        97   [dark, drawing, gimp, krita, male, man, mysterious, painting, (show more)]                                                                      
        101  [01, edit, kpop, mmd, sherlock, shinee, tda, vocaloid, (show more)]                                                                             
\end{Verbatim}
            \newpage
    \subsection{Final Dataframe}

In the final iteration of dataframe cleaning, the 'Ignored' columns were
removed. The 'downloads' column was also removed since it has only 4088
non-null data points.\\

'faves' is intended as the predicted variable. Although some of the
columns such as 'comments' and 'views' can't fairly be used in the
predictive process (since one does not know the number of views and
comments of an image beforehand), they are still retained for analysis
of their distributions and correlations.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{}final df}
        \PY{n}{final\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{account\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}critiques}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}deviations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}
        \PY{n}{rtist\PYZus{}faves}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}forum\PYZus{}posts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}page\PYZus{}views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}scraps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}watchers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artists}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{titles}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}links}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
        \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:}          account\_age  artist\_comments  artist\_critiques  artist\_deviations  \textbackslash{}
        0 1826 days 05:06:00  468.0            0.0               183.0               
        1 2921 days 22:33:36  169.0            0.0               36.0                
        2 2191 days 10:55:12  23.0             0.0               28.0                
        3 4748 days 03:39:36  322.0            0.0               55.0                
        4 2556 days 16:44:24  436.0            0.0               73.0                
        5 3652 days 10:12:00  17.0             0.0               77.0                
        6 2556 days 16:44:24  8264.0           0.0               150.0               
        7 4382 days 21:50:24  376.0            0.0               13.0                
        8 2191 days 10:55:12  128.0            0.0               97.0                
        9 2556 days 16:44:24  32.0             0.0               8.0                 
        
           artist\_faves  artist\_forum\_posts  artist\_page\_views  artist\_scraps  \textbackslash{}
        0  1830.0        0.0                 10634.0            0.0             
        1  216.0         0.0                 11377.0            2.0             
        2  53.0          0.0                 9907.0             0.0             
        3  396.0         0.0                 36431.0            3.0             
        4  37.0          0.0                 45119.0            0.0             
        5  6.0           0.0                 18474.0            0.0             
        6  3760.0        0.0                 68208.0            0.0             
        7  114.0         0.0                 6566.0             0.0             
        8  0.0           0.0                 29442.0            0.0             
        9  30.0          0.0                 2958.0             2.0             
        
           artist\_watchers          artists  comments   faves  \textbackslash{}
        0  54.0             get-sherlock     22.0      142.0    
        1  135.0            masterHalfling   45.0      1135.0   
        2  198.0            DonPerico        102.0     1959.0   
        3  1101.0           Cheeky-Bee       35.0      551.0    
        4  1118.0           HAHAAAAAAAAAAAA  8.0       91.0     
        5  47.0             1stClassStamps   21.0      696.0    
        6  1190.0           Feyjane          46.0      218.0    
        7  13.0             Elsias           5.0       96.0     
        8  706.0            SherlockTheGame  22.0      66.0     
        9  22.0             fractionofadot   38.0      877.0    
        
                                                 titles    views date\_posted  \textbackslash{}
        0  Sherlock                                      1606.0  2014-01-17    
        1  Sherlock                                      14153.0 2011-12-30    
        2  SHERLOCK                                      21163.0 2012-08-11    
        3  Sherlock Of the Dead                          6988.0  2012-01-21    
        4  Sherlock                                      2742.0  2011-08-19    
        5  Sherlock Holmes                               14566.0 2010-08-01    
        6  Sherlock                                      3139.0  2012-02-20    
        7  Sherlock                                      2208.0  2012-01-27    
        8  Sherlock: The Game Is On (Character Contest)  2983.0  2013-07-13    
        9  Sherlock                                      10189.0 2011-12-26    
        
           image\_age  \textbackslash{}
        0  1808.0      
        1  2557.0      
        2  2332.0      
        3  2535.0      
        4  2690.0      
        5  3073.0      
        6  2505.0      
        7  2529.0      
        8  1996.0      
        9  2561.0      
        
                                                                                                  image\_links  \textbackslash{}
        0  https://www.deviantart.com/get-sherlock/art/Sherlock-427568754                                       
        1  https://www.deviantart.com/masterhalfling/art/Sherlock-276657827                                     
        2  https://www.deviantart.com/donperico/art/SHERLOCK-320465780                                          
        3  https://www.deviantart.com/cheeky-bee/art/Sherlock-Of-the-Dead-280821239                             
        4  https://www.deviantart.com/hahaaaaaaaaaaaa/art/Sherlock-253839329                                    
        5  https://www.deviantart.com/1stclassstamps/art/Sherlock-Holmes-173600158                              
        6  https://www.deviantart.com/feyjane/art/Sherlock-286116362                                            
        7  https://www.deviantart.com/elsias/art/Sherlock-281791771                                             
        8  https://www.deviantart.com/sherlockthegame/art/Sherlock-The-Game-Is-On-Character-Contest-385061745   
        9  https://www.deviantart.com/fractionofadot/art/Sherlock-276038401                                     
        
                                                 image\_paths  \textbackslash{}
        0  full/c11741fda71c47368b3ecc42de439de62c48cf30.jpg   
        1  full/09fbbe05751a1b08cf03445e0821ca44dd5b547b.jpg   
        2  full/0f13dd8004eb7f0ce2b45c826f4e76df5978ac13.jpg   
        3  full/2d3083ae52b6ea7b69fdd83c0f74c5fd8525a489.jpg   
        4  full/be06d405f58195ef9d4c5c42c1c6cb2edb673754.jpg   
        5  full/11dc8450503f97976fba168972410295b76c72ed.jpg   
        6  full/587559fae93b85cbfa8802d39e7a2e1d3cbe415c.jpg   
        7  full/9507a844991ab37afec9bc0a0f619bfa0d033b04.jpg   
        8  full/45901b4a9092329595f2e6739d213b2d169e45f9.jpg   
        9  full/c752fa217f32f5510a49c4f17f3980db9a442c68.jpg   
        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   image\_urls  \textbackslash{}
        0  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/e9437807-b7f1-4c5b-a26b-f9e648379922/d72ka5u-e95f0672-311e-444d-91d1-fa2965b01f90.png/v1/fill/w\_1024,h\_537,q\_80,strp/sherlock\_by\_get\_sherlock\_d72ka5u-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NTM3IiwicGF0aCI6IlwvZlwvZTk0Mzc4MDctYjdmMS00YzViLWEyNmItZjllNjQ4Mzc5OTIyXC9kNzJrYTV1LWU5NWYwNjcyLTMxMWUtNDQ0ZC05MWQxLWZhMjk2NWIwMWY5MC5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.y4LtbGzQLxiWRZVSYC3OuJMVbv6XfvgSIGkGIzv47XU                                     
        1  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/dee153a3-1985-4a20-9c5a-8728f6aaf611/d4kpqjn-052b7481-dbd7-4e8a-bee2-465338941689.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2RlZTE1M2EzLTE5ODUtNGEyMC05YzVhLTg3MjhmNmFhZjYxMVwvZDRrcHFqbi0wNTJiNzQ4MS1kYmQ3LTRlOGEtYmVlMi00NjUzMzg5NDE2ODkuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.MGdgMPuuE2eZiv6yrT-gZ1d8CoLLdcNFLkxwOAnZs2A                                                                                                                                                                   
        2  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c0acf0b0-e7e6-4c7c-8ff6-7763aeb3dd79/d5asoz8-ec94075b-c573-45a5-8bb0-d52e49232e96.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2MwYWNmMGIwLWU3ZTYtNGM3Yy04ZmY2LTc3NjNhZWIzZGQ3OVwvZDVhc296OC1lYzk0MDc1Yi1jNTczLTQ1YTUtOGJiMC1kNTJlNDkyMzJlOTYuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.raEwrdO3gYAzPWV0BaonYhXWzcC3pAm-VMMFxdhLLec                                                                                                                                                                   
        3  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/bacca111-4256-4446-90e2-4aec411ca445/d4n6z1z-09894a11-50e9-4767-8804-f6dd4bc9c269.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2JhY2NhMTExLTQyNTYtNDQ0Ni05MGUyLTRhZWM0MTFjYTQ0NVwvZDRuNnoxei0wOTg5NGExMS01MGU5LTQ3NjctODgwNC1mNmRkNGJjOWMyNjkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.fKxTuklGbcGlYbIM0S\_AAkRVp9MDet\_yMD-42N8WWvg                                                                                                                                                                   
        4  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/75a9b8f3-6cde-4380-a50b-53a616ff3cbe/d474noh-8b31aa72-9e42-4006-afc0-1653023048d9.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzc1YTliOGYzLTZjZGUtNDM4MC1hNTBiLTUzYTYxNmZmM2NiZVwvZDQ3NG5vaC04YjMxYWE3Mi05ZTQyLTQwMDYtYWZjMC0xNjUzMDIzMDQ4ZDkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.MN8jGrrVbhVG6B5zmQsmyWaGcxPYSTDA7xIItuUn8Ig                                                                                                                                                                   
        5  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0725e00d-6f41-4f48-899c-5c8ed95814fe/d2vcuqm-25cee738-e834-4187-bd77-22a9dd88d8e3.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzA3MjVlMDBkLTZmNDEtNGY0OC04OTljLTVjOGVkOTU4MTRmZVwvZDJ2Y3VxbS0yNWNlZTczOC1lODM0LTQxODctYmQ3Ny0yMmE5ZGQ4OGQ4ZTMucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.OtN6CRHb7viL9pMiMrg0GvW4PCf3WxIyFp\_6gmhxHx4                                                                                                                                                                   
        6  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/4da4813c-23ac-4c24-a4af-948eaa298496/d4qcgsq-56275fbc-a0d4-4045-9a80-f77a2fb2c981.jpg/v1/fill/w\_900,h\_632,q\_75,strp/sherlock\_by\_feyjane\_d4qcgsq-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NjMyIiwicGF0aCI6IlwvZlwvNGRhNDgxM2MtMjNhYy00YzI0LWE0YWYtOTQ4ZWFhMjk4NDk2XC9kNHFjZ3NxLTU2Mjc1ZmJjLWEwZDQtNDA0NS05YTgwLWY3N2EyZmIyYzk4MS5qcGciLCJ3aWR0aCI6Ijw9OTAwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.NKBehw9Lh1lYIHt8tTQ\_jG08rgbSBEZ-WOskbTKxi60                                            
        7  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/06502dcd-0aae-4239-847b-a3355b8342d0/d4nrrx7-ec385512-9bea-46aa-8eed-3bb99981cc5d.jpg/v1/fill/w\_600,h\_770,q\_75,strp/sherlock\_by\_elsias\_d4nrrx7-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NzcwIiwicGF0aCI6IlwvZlwvMDY1MDJkY2QtMGFhZS00MjM5LTg0N2ItYTMzNTViODM0MmQwXC9kNG5ycng3LWVjMzg1NTEyLTliZWEtNDZhYS04ZWVkLTNiYjk5OTgxY2M1ZC5qcGciLCJ3aWR0aCI6Ijw9NjAwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.MFrdMPlD0MCVREsbhl0-bc6r4QQTcx1jW47a\_e3hnq8                                             
        8  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/bff32934-e726-4d2c-85f9-2ed211cc4e46/d6d97jl-7b5d62ec-1167-49a3-aff4-1da141868ab6.png/v1/fill/w\_1024,h\_750,strp/sherlock\_\_the\_game\_is\_on\_\_character\_contest\_\_by\_sherlockthegame\_d6d97jl-fullview.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NzUwIiwicGF0aCI6IlwvZlwvYmZmMzI5MzQtZTcyNi00ZDJjLTg1ZjktMmVkMjExY2M0ZTQ2XC9kNmQ5N2psLTdiNWQ2MmVjLTExNjctNDlhMy1hZmY0LTFkYTE0MTg2OGFiNi5wbmciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.8FJFQbkoZhTxoISgGnPk7nl7lDh3lTbB5JU0qlZSD5c   
        9  https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c5b2ce3d-fee1-43ef-b7cd-6c8fea631e79/d4kcgld-a37fe0a1-67da-437f-9065-924a4776a849.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2M1YjJjZTNkLWZlZTEtNDNlZi1iN2NkLTZjOGZlYTYzMWU3OVwvZDRrY2dsZC1hMzdmZTBhMS02N2RhLTQzN2YtOTA2NS05MjRhNDc3NmE4NDkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.eqBpfLqmpgdZvMgTw11YDY6ul6pJZmsFWbydy-WOZI8                                                                                                                                                                   
        
                                          artist\_urls  
        0  https://www.deviantart.com/get-sherlock     
        1  https://www.deviantart.com/masterhalfling   
        2  https://www.deviantart.com/donperico        
        3  https://www.deviantart.com/cheeky-bee       
        4  https://www.deviantart.com/hahaaaaaaaaaaaa  
        5  https://www.deviantart.com/1stclassstamps   
        6  https://www.deviantart.com/feyjane          
        7  https://www.deviantart.com/elsias           
        8  https://www.deviantart.com/sherlockthegame  
        9  https://www.deviantart.com/fractionofadot   
\end{Verbatim}
            
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\newpage
    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5638 entries, 0 to 5637
Data columns (total 20 columns):
account\_age           5621 non-null timedelta64[ns]
artist\_comments       5621 non-null float64
artist\_critiques      5621 non-null float64
artist\_deviations     5621 non-null float64
artist\_faves          5621 non-null float64
artist\_forum\_posts    5621 non-null float64
artist\_page\_views     5621 non-null float64
artist\_scraps         5621 non-null float64
artist\_watchers       5621 non-null float64
artists               5638 non-null object
comments              5629 non-null float64
faves                 5636 non-null float64
titles                5638 non-null object
views                 5636 non-null float64
date\_posted           5624 non-null datetime64[ns]
image\_age             5624 non-null float64
image\_links           5638 non-null object
image\_paths           5638 non-null object
image\_urls            5638 non-null object
artist\_urls           5621 non-null object
dtypes: datetime64[ns](1), float64(12), object(6), timedelta64[ns](1)
memory usage: 881.0+ KB

    \end{Verbatim}

    \subsection{Filling NaNs with Column Means}

As a final step in the cleaning process, all missing values are filled
in with the column mean. This leaves 14 NaN values in the date\_posted
column and 17 NaN values in the artist\_urls columns since there is no
"mean" for these columns. These NaN values can be safely ignored since
the columns will not be used in the regression analysis.
\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of Nans in DataFrame:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{final\PYZus{}df.isna().sum()\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}fill all NaN values with the column\PYZsq{}s mean amount}
         \PY{n}{final\PYZus{}df} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Number of Nans in DataFrame:

account\_age           17
artist\_comments       17
artist\_critiques      17
artist\_deviations     17
artist\_faves          17
artist\_forum\_posts    17
artist\_page\_views     17
artist\_scraps         17
artist\_watchers       17
artists               0
comments              9
faves                 2
titles                0
views                 2
date\_posted           14
image\_age             14
image\_links           0
image\_paths           0
image\_urls            0
artist\_urls           17
dtype: int64

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Final Number of Nans in DataFrame after filling missing values with column}
         \PY{n}{mean}\PY{p}{:}\PYZbs{}\PY{n}{n}\PYZbs{}\PY{n}{n}\PY{p}{\PYZob{}}\PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{isna}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Final Number of Nans in DataFrame after filling missing values with column mean:

account\_age           0
artist\_comments       0
artist\_critiques      0
artist\_deviations     0
artist\_faves          0
artist\_forum\_posts    0
artist\_page\_views     0
artist\_scraps         0
artist\_watchers       0
artists               0
comments              0
faves                 0
titles                0
views                 0
date\_posted           14
image\_age             0
image\_links           0
image\_paths           0
image\_urls            0
artist\_urls           17
dtype: int64

    \end{Verbatim}
\newpage
    \section{Data Analysis}

The following analyses wil be conducted to gain a better understanding
of the data that is being handled:

\begin{itemize}
\item
  Analyzing descriptive statistics.
\item
  Analyzing the distributions and correlations in the data.
\item
  Analyzing any variables which contain a time-series component. In this
  case, only date\_posted has a time-series component.
\end{itemize}

    \subsection{Descriptive Statistics}

The descriptive statistics dataframes below allow a more detailed
insight into the data.\footnote{\textbf{\#descriptivestats:} Analyzing descriptive statistics of the data is an important step to gain a better understanding of the data, unveil insights, and detect anomalies. In this case, analyzing the descriptive statistics of the artist attribute columns allowed me to point out some salient issues, and also provided insight into the missing artist links. A brief glance at the mean, min, and max also provides us with a good outline of the basic statistics of the data.} Some of the more salient issues are listed below:

\begin{itemize}
\tightlist
\item
  The artist\_critiques, artist\_forum\_posts, and artist\_scraps
  columns contain a lot of zeros. This suggests that most people on
  DeviantArt - or at least those within the current sample - do not make
  much use of these functionalities. The lack of use of these
  functionalities can be explained below:

  \begin{itemize}
  \tightlist
  \item
    'artist\_critique': The regular user would use DeviantArt to browse
    art, comment, and favorite it. Making a critique takes additional
    effort since one has to adhere to several guidelines to do so. \\
  \item
    'artist\_forum\_posts': Forums are not that popular since most
    people come to the site to view and make art vs having discussions. \\
  \item
    'artist\_scraps': Most users aren't inclined to post their
    works-in-progress or to archive their art in "Scraps", and instead
    choose to leave everything in the main gallery.
  \end{itemize}
\item
  There are 17 banned artists, which explains the missing artist links
  above.
\end{itemize}
\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:}                      account\_age  artist\_comments  artist\_critiques  \textbackslash{}
         count  5638                       5638.00          5638.00            
         mean   3283 days 18:38:14.697384  4959.48          0.26               
         std    1029 days 09:17:57.753093  16813.99         1.47               
         min    152 days 04:25:30          0.00             0.00               
         25\%    2556 days 16:44:24         384.25           0.00               
         50\%    3287 days 04:22:48         1481.50          0.00               
         75\%    4017 days 16:01:12         4812.50          0.00               
         max    6574 days 08:45:36         987561.00        34.00              
         
                artist\_deviations  artist\_faves  artist\_forum\_posts  artist\_page\_views  \textbackslash{}
         count  5638.00            5638.00       5638.00             5638.00             
         mean   308.08             3889.57       57.73               113944.25           
         std    713.35             18723.56      1112.49             801727.25           
         min    0.00               0.00          0.00                336.00              
         25\%    69.25              228.25        0.00                11109.00            
         50\%    164.00             869.00        0.00                25035.50            
         75\%    344.00             2887.75       2.00                64305.25            
         max    34267.00           906201.00     79536.00            44584418.00         
         
                artist\_scraps  artist\_watchers  comments     faves      views  \textbackslash{}
         count  5638.00        5638.00          5638.00   5638.00   5638.00     
         mean   20.35          1567.65          23.85     145.73    3375.18     
         std    141.22         9501.20          43.72     396.01    7041.45     
         min    0.00           0.00             0.00      7.00      22.00       
         25\%    0.00           55.00            5.00      33.00     687.00      
         50\%    1.00           208.00           12.00     57.00     1435.00     
         75\%    12.00          821.00           26.00     127.00    3204.50     
         max    8678.00        555719.00        1374.00   13066.00  165273.00   
         
                image\_age  
         count  5638.00    
         mean   2225.46    
         std    658.55     
         min    0.00       
         25\%    1824.00    
         50\%    2345.00    
         75\%    2539.00    
         max    5568.00    
\end{Verbatim}
            \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{O}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:}        artists    titles  \textbackslash{}
         count   5638    5638       
         unique  5622    4073       
         top     Banned  Sherlock   
         freq    17      644        
         
                                                                              image\_links  \textbackslash{}
         count   5638                                                                       
         unique  5638                                                                       
         top     https://www.deviantart.com/jbshlover/art/JB-as-Sherlock-Holmes-163778585   
         freq    1                                                                          
         
                                                       image\_paths  \textbackslash{}
         count   5638                                                
         unique  5638                                                
         top     full/ffcdbdde1b0ff5e313500a82eb1f98193bbb0f54.jpg   
         freq    1                                                   
         
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              image\_urls  \textbackslash{}
         count   5638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
         unique  5638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
         top     https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/33c52cee-2f10-42d7-9f4d-bf4a86cfd680/d5fcm0r-e41f6c05-7fef-4fe5-9b0a-d90a97a27d66.png/v1/fill/w\_900,h\_673,q\_80,strp/sherlock\_and\_watson\_by\_tagoston16\_d5fcm0r-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NjczIiwicGF0aCI6IlwvZlwvMzNjNTJjZWUtMmYxMC00MmQ3LTlmNGQtYmY0YTg2Y2ZkNjgwXC9kNWZjbTByLWU0MWY2YzA1LTdmZWYtNGZlNS05YjBhLWQ5MGE5N2EyN2Q2Ni5wbmciLCJ3aWR0aCI6Ijw9OTAwIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.aAUSmP2Wi0yoYtwGKHNaPJH2gAXjJx9YEcjgD00hQM4   
         freq    1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
         
                                            artist\_urls  
         count   5621                                    
         unique  5621                                    
         top     https://www.deviantart.com/leafbreeze7  
         freq    1                                       
\end{Verbatim}
                        \newpage
    \subsection{Distributions and Correlations}

\subsubsection{Predictor Features}

The final predictors to be used in the regression analysis are
'artist\_comments', 'artist\_critiques', 'artist\_deviations',
'artist\_faves', 'artist\_forum\_posts', 'artist\_page\_views',
'artist\_scraps', and 'artist\_watchers'. 'faves' - the number of image
favorites - is used as a proxy for popularity and represents the
response variable.\\

An unnormalized pairplot reveals that all of the predictors - and the
response variables - have an exponential distribution. As such, all the
variables are log-normalized so that there is less skew in their
distributions. This should improve the model-training process since the
model will not be misled by stark outliers.\footnote{\textbf{\#distributions:} By analyzing the distributions of the data, we can quickly see whether the data is normal, heavily skewed, etc. In this case, all the predictor columns - including the response column - turn out to be exponentially distributed, and so the data was log-normalized to resemble normal distributions instead. Analyzing distributions and normalizing those that are heavily skewed are vital steps in the machine learning process to help increase the model's accuracy.}\\

The log-normalized pairplot shows that most of the distributions have
been successfully normalized to resemble a Gaussian distribution,
although the artist\_critiques, forum\_posts, and artist\_scraps still
skew heavily to 0, as expected due to the large number of zeros within
those columns.
\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}predictors are features that the artist knows before posting an image (faves is also}
         \PY{n}{included} \PY{k}{as} \PY{n}{the} \PY{n}{response} \PY{n}{variable}\PY{p}{)}
         \PY{n}{predictors} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}critiques}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}deviations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}
         \PY{n}{rtist\PYZus{}forum\PYZus{}posts}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}page\PYZus{}views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}scraps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}watchers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{n}{predictors}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} <seaborn.axisgrid.PairGrid at 0x1a170aedd8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_29_1.png}
    \end{center}

    \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{log\PYZus{}predictors\PYZus{}df} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{n}{predictors}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{log\PYZus{}predictors\PYZus{}df}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} <seaborn.axisgrid.PairGrid at 0x1a18d28b38>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_30_1.png}
    \end{center}
     \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{corr} \PY{o}{=} \PY{n}{log\PYZus{}predictors\PYZus{}df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr}\PY{p}{,}
                     \PY{n}{xticklabels}\PY{o}{=}\PY{n}{corr}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                     \PY{n}{yticklabels}\PY{o}{=}\PY{n}{corr}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                     \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{annot\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{15}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1a26cdbb70>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_31_1.png}
    \end{center}
    \newpage
    \subsubsection{Correlation Heatmap Analysis}

\begin{itemize}
\item
  The number of watchers and total page views has the highest
  correlations with the number of favorites received by a particular
  image, with correlation coefficients of 0.43 and 0.38 respectively.
  This makes sense since the more watchers (aka followers) an artist has
  and the higher the frequency their account page is viewed, the higher
  the probability of them being a "popular" artist whose art is
  appreciated and favorited.
\item
  Although the number of deviations (images posted) has a high
  correlation with number of page views (0.73), the correlation between
  number of deviations and number of favorites on a given image is only
  0.045. This suggests that when artists produce a lot of artwork, their
  pages tend to be viewed very often, but this doesn't necessarily mean
  that people like their work, i.e. quantity does not mean quality.
\item
  The number of total favorites received by an artist across all their
  work has a weak correlation (0.05) with the number of favorites for a
  given image. This is probably because although an artist has a high
  amount of total favorites, it could be due to them accruing favorites
  through image quantity. More deviations - albeit with less favorites
  per deviation - could possibly accrue as many likes as a handful of
  high-favorite deviations.
\end{itemize}

There are many other insights to be gathered from the heatmap, and most
make intuitive sense as to what one would expect from the data.
    \newpage
    \subsubsection{Other Features}

Other features are social metrics that should not be used for prediction
since they are unknown at the time of image posting, but are nonetheless
interesting to examine for insights. These consist of the number of
comments, number of views, the image age, and once again, the number of
image favorites.\\

The pairplots show that the comments, views, and favorites and
exponentially distribution, while the image age is normally distributed.
As such, the comments, views, and favorites are log-normalized to
resemble Gaussian distributions before building the correlation heatmap.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{}these other features are only known after posting}
         \PY{n}{others} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{n}{others}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} <seaborn.axisgrid.PairGrid at 0x1a2733f438>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_34_1.png}
    \end{center}
    
    \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{}only log comments, faves, and views since image\PYZus{}age is already normally distributed}
         \PY{n}{log\PYZus{}others\PYZus{}df} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{log\PYZus{}others\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{log\PYZus{}others\PYZus{}df}\PY{p}{,} \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{log\PYZus{}others\PYZus{}df}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} <seaborn.axisgrid.PairGrid at 0x1a29acc4e0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_35_1.png}
    \end{center}

    \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{corr} \PY{o}{=} \PY{n}{log\PYZus{}others\PYZus{}df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr}\PY{p}{,}
                     \PY{n}{xticklabels}\PY{o}{=}\PY{n}{corr}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                     \PY{n}{yticklabels}\PY{o}{=}\PY{n}{corr}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                     \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{annot\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{12}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1a3cf08ac8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_36_1.png}
    \end{center}

    
    \subsubsection{Correlation Heatmap Analysis}

The correlation heatmap shows that there are high correlations between
comments, views, and faves; and a moderate correlation between image age
and views, but low correlations between image age and comments and
faves. This suggests that although older images tend to accrue views
over time, this does not necessarily translate to additional comments
and favorites.

\newpage
    \subsection{Time-series of Date Posted}

The time-series of the dates the images were posted are analyzed to see
whether there are any significant time-dependent trends within the data.
All observations conducted are constrained in their generalizability due
to the small sample size of data (5638 images).

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}203}]:} \PY{c+c1}{\PYZsh{}create month\PYZhy{}year column to get monthly time periods}
          \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{month}
          \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{year\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{year}
          \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month\PYZus{}year\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{dt}\PY{o}{.}\PY{n}{to\PYZus{}period}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{k}{def} \PY{n+nf}{create\PYZus{}time\PYZus{}series}\PY{p}{(}\PY{n}{timescale}\PY{p}{,} \PY{n}{title}\PY{p}{,} \PY{n}{label\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{14}\PY{p}{,} \PY{n}{title\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,} \PY{n}{x\PYZus{}tick\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{13}\PY{p}{,}
          \PY{n}{side\PYZus{}plots} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
          
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Inputs:}
          \PY{l+s+sd}{        timescale (str): name of timescale column to be used in groupby function}
          \PY{l+s+sd}{                         to get average faves per post and number of posts}
          
          \PY{l+s+sd}{        title (str): name of timescale to appear in the title and x\PYZhy{}axis label}
          
          \PY{l+s+sd}{        label\PYZus{}size (int): x\PYZhy{}axis and y\PYZhy{}axis font size. Default: 13}
          
          \PY{l+s+sd}{        title\PYZus{}size (int): title font size. Default: 15}
          
          \PY{l+s+sd}{        x\PYZus{}tick\PYZus{}size (int): x\PYZhy{}ticks font size. Default: 13}
          
          \PY{l+s+sd}{        side\PYZus{}plots (bool): if True, plots will be displayed side by side.}
          \PY{l+s+sd}{                            If False, plots will be displayed top and bottom.}
          \PY{l+s+sd}{                            Default: True}
          
          \PY{l+s+sd}{    Outputs:}
          \PY{l+s+sd}{        Two bar plots showing the average number of faves and number of posts}
          \PY{l+s+sd}{        over the given timescale.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{k}{if} \PY{n}{side\PYZus{}plots}\PY{p}{:}
                  \PY{n}{a} \PY{o}{=} \PY{l+m+mi}{221}
                  \PY{n}{b} \PY{o}{=} \PY{l+m+mi}{222}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{a} \PY{o}{=} \PY{l+m+mi}{211}
                  \PY{n}{b} \PY{o}{=} \PY{l+m+mi}{212}
          
              \PY{c+c1}{\PYZsh{}make df of average faves and number of posts over timescale}
              \PY{n}{avg\PYZus{}faves} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{n}{timescale}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
              \PY{n}{no\PYZus{}of\PYZus{}posts} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{n}{timescale}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{}plot the dfs}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{a}\PY{p}{)}
              \PY{n}{avg\PYZus{}faves}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average Number of Faves per }\PY{l+s+si}{\PYZob{}title\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{title\PYZus{}size}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}title\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{label\PYZus{}size}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{fontsize}\PY{o}{=}\PY{n}{x\PYZus{}tick\PYZus{}size}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Average number of faves}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{label\PYZus{}size}\PY{p}{)}
          
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{b}\PY{p}{)}
              \PY{n}{no\PYZus{}of\PYZus{}posts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Total Number of Posts per }\PY{l+s+si}{\PYZob{}title\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{title\PYZus{}size}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}title\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{label\PYZus{}size}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{fontsize}\PY{o}{=}\PY{n}{x\PYZus{}tick\PYZus{}size}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{90}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of posts}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize}\PY{o}{=}\PY{n}{label\PYZus{}size}\PY{p}{)}
\end{Verbatim}

\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}204}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{create\PYZus{}time\PYZus{}series}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Month}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_40_0.png}
    \end{center}

    
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}205}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{create\PYZus{}time\PYZus{}series}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{year\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_41_0.png}
    \end{center}

    \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}206}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{25}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{)}
          \PY{n}{create\PYZus{}time\PYZus{}series}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month\PYZus{}year\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Month\PYZhy{}Year}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{title\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{27}\PY{p}{,}
          \PY{n}{x\PYZus{}tick\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{,} \PY{n}{side\PYZus{}plots}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_42_0.png}
    \end{center}

    \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}25}]:} \PY{k+kn}{from} \PY{n+nn}{datetime} \PY{k}{import} \PY{n}{datetime}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{as} \PY{n+nn}{mpimg}
         \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{core}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{HTML}
         
         \PY{c+c1}{\PYZsh{}find the outlier}
         \PY{n}{outlier\PYZus{}date} \PY{o}{=} \PY{n}{datetime}\PY{o}{.}\PY{n}{strptime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2008\PYZhy{}10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{Y\PYZhy{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{outlier\PYZus{}df} \PY{o}{=}
         \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artists}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{titles}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}links}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{p}{(}
             \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{year\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{outlier\PYZus{}date}\PY{o}{.}\PY{n}{year}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{month\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==}
         \PY{n}{outlier\PYZus{}date}\PY{o}{.}\PY{n}{month}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n}{display}\PY{p}{(}\PY{n}{HTML}\PY{p}{(}\PY{n}{outlier\PYZus{}df}\PY{o}{.}\PY{n}{to\PYZus{}html}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{outlier\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}links}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{img} \PY{o}{=} \PY{n}{mpimg}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DA\PYZhy{}images/}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full/cf5fd090c1f8cb96bef89b500abb6974e66cb600.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}

    
    \begin{verbatim}
<IPython.core.display.HTML object>
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
5498    https://www.deviantart.com/halconrojo2006/art/Sherlock-Holmes-Museum-
London-101940689
5587    https://www.deviantart.com/hideyoshi/art/Sherlock-Holmes-99996840
Name: image\_links, dtype: object

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_43_2.png}
    \end{center}

        \newpage
    \subsubsection{Time-series Analysis}

    \subsubsection*{Number of posts}

From the time-series plots, we see that the posts are distributed from
the years 2003 to 2018. The highest number of posts was in 2012, and
this is reflected in the month-year chart as well, where we see that
2012-01 has the highest number of posts; the timeframe from 2012 - 2014
in general appears to contribute the most amont of posts. This aligns
with the Google Trends graph which showed that interest in the 'Sherlock
Holmes' term peaked worldwide in 2012 and 2014, which is pretty neat. In
my graph, we also start to see that the number of posts start to
increase around 2010, which was when the Sherlock Holmes movie and tv
series were both high in popularity. This suggests that there are a good
amount of fans who produce fanart when they gain interest in a show.\\

Month-wise, it appears that the highest number of posts is in January.
This could simply be due to the show's popularity around January 2012,
which skewed the rest of the January data.

   \subsubsection*{Average number of favorites - finding the outlier}

The average number of favorites across time appear to be relatively
similar, though there is one huge outlier on the date of 2008-10. A
closer examination of this particular date reveals that there were only
2 images posted on this date, but as one of them accrued a high amount
of favorites of 3462, the average number of favorites per post is skewed
on that date.
\newpage
    \section{Model-Building}

\subsection{Predicting number of favorites based on social metrics}

As explained earlier, various linear regression models and a neural
network model are trained to predict the number of favorites based on
social metrics. The dataset is split with a test-set ratio of 0.2,
giving us 4510 training samples and 1128 testing samples.

\subsubsection{Feature selection}

Sklearn's Recursive Feature Elimination (RFE) function is used to select
features by recursively considering smaller and smaller sets of features
and their effect on the model's performance. The importance of each
feature is obtained using the coef\_ function. I used the
\texttt{RFECV()} function as it has inbuilt cross-validation to
determine the optimum number of features that should be inlcuded when
training the model (vs having to choose the number of features myself
using the basic \texttt{RFE()} function).\\

Interestingly, the \texttt{RFECV()} function shows that all the features
are important for model performance (Rank 1). This is especially
interesting considering that when I did this earlier with a smaller
sample size (3000 points), at least one or two features would get
dropped. This suggests that as more data is collected, the predictive
power of the features, even when small, are still able to contribute
towards improving the model's accuracy.

\subsubsection{Linear regression models}

The linear regression models used were OLS regression, ridge, lasso and
elastic net regression. Ridge, lasso, and elastic net regression were
cross-validated using the inbuilt sklearn CV functions. Since OLS
regression did not come with an inbuilt CV function, I ommited the
cross-validation step on the basis that the other regressions should be
more accurate due to being less prone to overfitting anyway, and so I
was more concerned with their performance.

\subsubsection{Neural network}

A very simple neural network consisting of 1 input layer, 2 hidden
layers, and 1 output layer was built. The first 3 layers had "relu"
activation functions to learn any non-linearities within the data. The
output layer had a "linear" activation function so that a continuous
value for regression purposes was produced. The total trainable
parameters was 321, and the model was trained with a mean squared error
loss and adam optimizer for 100 epochs, with a batch size of 20 and
validation split of 0.2.

\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}178}]:} \PY{c+c1}{\PYZsh{}linear models for social metric prediction}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}\PY{p}{,} \PY{n}{RidgeCV}\PY{p}{,} \PY{n}{LassoCV}\PY{p}{,} \PY{n}{ElasticNetCV}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
          \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}selection} \PY{k}{import} \PY{n}{RFECV} \PY{c+c1}{\PYZsh{}recursive feature elimination with cross\PYZhy{}}
          \PY{n}{validation}
          
          \PY{n}{X\PYZus{}log} \PY{o}{=} \PY{n}{log\PYZus{}predictors\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{n}{y\PYZus{}log} \PY{o}{=} \PY{n}{log\PYZus{}predictors\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}log}\PY{p}{,} \PY{n}{y\PYZus{}log}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
          \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
(4510, 8) (1128, 8) (4510,) (1128,)

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}179}]:} \PY{k}{def} \PY{n+nf}{selector\PYZus{}model}\PY{p}{(}\PY{n}{estimator}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{:}
          
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Inputs:}
          \PY{l+s+sd}{        X\PYZus{}train (arr): Feature array of data.}
          \PY{l+s+sd}{        y\PYZus{}train (arr): Response array of data.}
          
          \PY{l+s+sd}{    Outputs:}
          \PY{l+s+sd}{        selector: estimator with feature selection applied}
          
          \PY{l+s+sd}{    Attributes:}
          \PY{l+s+sd}{        ranking\PYZus{}: The feature ranking, such that ranking\PYZus{}[i] corresponds to the ranking position of the i\PYZhy{}th feature. Selected (i.e., estimated best) features are assigned rank 1.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
          
              \PY{n}{selector} \PY{o}{=} \PY{n}{RFECV}\PY{p}{(}\PY{n}{estimator}\PY{p}{,} \PY{n}{step} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}
          \PY{n}{y\PYZus{}train}\PY{p}{)}
              \PY{k}{return} \PY{n}{selector}
          
          \PY{k}{def} \PY{n+nf}{align\PYZus{}coefficients}\PY{p}{(}\PY{n}{selector}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Simple function to adjust coefficient layout in dataframe to correspond with}
          \PY{l+s+sd}{    selected features from the given selector.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{n}{j} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{selector}\PY{o}{.}\PY{n}{ranking\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                  \PY{k}{if} \PY{n}{selector}\PY{o}{.}\PY{n}{ranking\PYZus{}}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{!=} \PY{l+m+mi}{1}\PY{p}{:}
                      \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{nan}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{coefs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{selector}\PY{o}{.}\PY{n}{estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}
                      \PY{n}{j} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
              \PY{k}{return} \PY{n}{coefs}
          
          \PY{c+c1}{\PYZsh{}linear regressions}
          \PY{n}{linear\PYZus{}reg} \PY{o}{=} \PY{n}{selector\PYZus{}model}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{p}{)}
          \PY{n}{ridge\PYZus{}reg} \PY{o}{=} \PY{n}{selector\PYZus{}model}\PY{p}{(}\PY{n}{RidgeCV}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{p}{)}
          \PY{n}{lasso\PYZus{}reg} \PY{o}{=} \PY{n}{selector\PYZus{}model}\PY{p}{(}\PY{n}{LassoCV}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{p}{)}
          
          \PY{n}{feature\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{X\PYZus{}log}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression Feature Ranking}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{linear\PYZus{}reg}\PY{o}{.}\PY{n}{ranking\PYZus{}}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression Coefficients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
          \PY{n}{align\PYZus{}coefficients}\PY{p}{(}\PY{n}{linear\PYZus{}reg}\PY{p}{)}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ridge Regression Feature Ranking}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{ridge\PYZus{}reg}\PY{o}{.}\PY{n}{ranking\PYZus{}}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ridge Regression Coefficients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
          \PY{n}{align\PYZus{}coefficients}\PY{p}{(}\PY{n}{ridge\PYZus{}reg}\PY{p}{)}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lasso Regression Feature Ranking}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{lasso\PYZus{}reg}\PY{o}{.}\PY{n}{ranking\PYZus{}}\PY{p}{,}
                                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lasso Regression Coefficients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
          \PY{n}{align\PYZus{}coefficients}\PY{p}{(}\PY{n}{lasso\PYZus{}reg}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
          
          \PY{n}{feature\PYZus{}df}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/feature-ranking-coef.png}
    \end{center}
            
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}180}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
          \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Input}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}
          
          \PY{k}{def} \PY{n+nf}{create\PYZus{}mlp}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} define our MLP network}
              \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{n}{dim}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{relu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
              \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
          
              \PY{k}{return} \PY{n}{model}
          
          \PY{n}{nn\PYZus{}model} \PY{o}{=} \PY{n}{create\PYZus{}mlp}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          \PY{n}{nn\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#
=================================================================
dense\_14 (Dense)             (None, 16)                144
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_15 (Dense)             (None, 8)                 136
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_16 (Dense)             (None, 4)                 36
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_17 (Dense)             (None, 1)                 5
=================================================================
Total params: 321
Trainable params: 321
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}181}]:} \PY{n}{nn\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mse}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mae}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{nn\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{validation\PYZus{}split} \PY{o}{=} \PY{l+m+mf}{0.2}\PY{p}{,}
          \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Train on 3608 samples, validate on 902 samples
Epoch 1/100
3608/3608 [==============================] - 1s 218us/step - loss: 7.6398 -
mean\_absolute\_error: 2.2461 - mean\_squared\_error: 7.6398 -
mean\_absolute\_percentage\_error: 52.4107 - val\_loss: 1.1765 - val\_mean\_absolute\_error:
0.8438 - val\_mean\_squared\_error: 1.1765 - val\_mean\_absolute\_percentage\_error: 20.1149
Epoch 2/100
3608/3608 [==============================] - 0s 75us/step - loss: 1.0149 -
mean\_absolute\_error: 0.7847 - mean\_squared\_error: 1.0149 -
mean\_absolute\_percentage\_error: 18.9972 - val\_loss: 0.7903 - val\_mean\_absolute\_error:
0.6935 - val\_mean\_squared\_error: 0.7903 - val\_mean\_absolute\_percentage\_error: 16.9284
Epoch 3/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.8754 -
mean\_absolute\_error: 0.7358 - mean\_squared\_error: 0.8754 -
mean\_absolute\_percentage\_error: 17.9443 - val\_loss: 0.7399 - val\_mean\_absolute\_error:
0.6590 - val\_mean\_squared\_error: 0.7399 - val\_mean\_absolute\_percentage\_error: 15.5673
Epoch 4/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.8609 -
mean\_absolute\_error: 0.7259 - mean\_squared\_error: 0.8609 -
mean\_absolute\_percentage\_error: 17.7648 - val\_loss: 0.7258 - val\_mean\_absolute\_error:
0.6495 - val\_mean\_squared\_error: 0.7258 - val\_mean\_absolute\_percentage\_error: 15.2035
Epoch 5/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.8373 -
mean\_absolute\_error: 0.7146 - mean\_squared\_error: 0.8373 -
mean\_absolute\_percentage\_error: 17.4934 - val\_loss: 0.6828 - val\_mean\_absolute\_error:
0.6449 - val\_mean\_squared\_error: 0.6828 - val\_mean\_absolute\_percentage\_error: 15.8655
Epoch 6/100
3608/3608 [==============================] - 0s 78us/step - loss: 0.8184 -
mean\_absolute\_error: 0.7091 - mean\_squared\_error: 0.8184 -
mean\_absolute\_percentage\_error: 17.3911 - val\_loss: 0.6933 - val\_mean\_absolute\_error:
0.6593 - val\_mean\_squared\_error: 0.6933 - val\_mean\_absolute\_percentage\_error: 16.5276
Epoch 7/100
3608/3608 [==============================] - 0s 69us/step - loss: 0.8242 -
mean\_absolute\_error: 0.7117 - mean\_squared\_error: 0.8242 -
mean\_absolute\_percentage\_error: 17.4397 - val\_loss: 0.6864 - val\_mean\_absolute\_error:
0.6567 - val\_mean\_squared\_error: 0.6864 - val\_mean\_absolute\_percentage\_error: 16.5001
Epoch 8/100
3608/3608 [==============================] - 0s 65us/step - loss: 0.8222 -
mean\_absolute\_error: 0.7113 - mean\_squared\_error: 0.8222 -
mean\_absolute\_percentage\_error: 17.4590 - val\_loss: 0.6669 - val\_mean\_absolute\_error:
0.6404 - val\_mean\_squared\_error: 0.6669 - val\_mean\_absolute\_percentage\_error: 15.8930
Epoch 9/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.8096 -
mean\_absolute\_error: 0.7049 - mean\_squared\_error: 0.8096 -
mean\_absolute\_percentage\_error: 17.3490 - val\_loss: 0.6611 - val\_mean\_absolute\_error:
0.6264 - val\_mean\_squared\_error: 0.6611 - val\_mean\_absolute\_percentage\_error: 15.0521
Epoch 10/100
3608/3608 [==============================] - 0s 70us/step - loss: 0.8108 -
mean\_absolute\_error: 0.7042 - mean\_squared\_error: 0.8108 -
mean\_absolute\_percentage\_error: 17.3184 - val\_loss: 0.7347 - val\_mean\_absolute\_error:
0.6529 - val\_mean\_squared\_error: 0.7347 - val\_mean\_absolute\_percentage\_error: 14.9523
Epoch 11/100
3608/3608 [==============================] - 0s 84us/step - loss: 0.8051 -
mean\_absolute\_error: 0.7018 - mean\_squared\_error: 0.8051 -
mean\_absolute\_percentage\_error: 17.2520 - val\_loss: 0.7396 - val\_mean\_absolute\_error:
0.6553 - val\_mean\_squared\_error: 0.7396 - val\_mean\_absolute\_percentage\_error: 14.9820
Epoch 12/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.8139 -
mean\_absolute\_error: 0.7057 - mean\_squared\_error: 0.8139 -
mean\_absolute\_percentage\_error: 17.3485 - val\_loss: 0.6608 - val\_mean\_absolute\_error:
0.6380 - val\_mean\_squared\_error: 0.6608 - val\_mean\_absolute\_percentage\_error: 15.8114
Epoch 13/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7948 -
mean\_absolute\_error: 0.6971 - mean\_squared\_error: 0.7948 -
mean\_absolute\_percentage\_error: 17.1241 - val\_loss: 0.6534 - val\_mean\_absolute\_error:
0.6285 - val\_mean\_squared\_error: 0.6534 - val\_mean\_absolute\_percentage\_error: 15.3458
Epoch 14/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.8038 -
mean\_absolute\_error: 0.7030 - mean\_squared\_error: 0.8038 -
mean\_absolute\_percentage\_error: 17.2636 - val\_loss: 0.6590 - val\_mean\_absolute\_error:
0.6363 - val\_mean\_squared\_error: 0.6590 - val\_mean\_absolute\_percentage\_error: 15.7888
Epoch 15/100
3608/3608 [==============================] - 0s 86us/step - loss: 0.8076 -
mean\_absolute\_error: 0.7029 - mean\_squared\_error: 0.8076 -
mean\_absolute\_percentage\_error: 17.2692 - val\_loss: 0.6752 - val\_mean\_absolute\_error:
0.6301 - val\_mean\_squared\_error: 0.6752 - val\_mean\_absolute\_percentage\_error: 14.8686
Epoch 16/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7935 -
mean\_absolute\_error: 0.6968 - mean\_squared\_error: 0.7935 -
mean\_absolute\_percentage\_error: 17.1416 - val\_loss: 0.6814 - val\_mean\_absolute\_error:
0.6317 - val\_mean\_squared\_error: 0.6814 - val\_mean\_absolute\_percentage\_error: 14.8395
Epoch 17/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.8006 -
mean\_absolute\_error: 0.7002 - mean\_squared\_error: 0.8006 -
mean\_absolute\_percentage\_error: 17.1774 - val\_loss: 0.6596 - val\_mean\_absolute\_error:
0.6380 - val\_mean\_squared\_error: 0.6596 - val\_mean\_absolute\_percentage\_error: 15.8298
Epoch 18/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.8039 -
mean\_absolute\_error: 0.7021 - mean\_squared\_error: 0.8039 -
mean\_absolute\_percentage\_error: 17.2947 - val\_loss: 0.6581 - val\_mean\_absolute\_error:
0.6244 - val\_mean\_squared\_error: 0.6581 - val\_mean\_absolute\_percentage\_error: 14.9656
Epoch 19/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7999 -
mean\_absolute\_error: 0.7017 - mean\_squared\_error: 0.7999 -
mean\_absolute\_percentage\_error: 17.2280 - val\_loss: 0.6524 - val\_mean\_absolute\_error:
0.6294 - val\_mean\_squared\_error: 0.6524 - val\_mean\_absolute\_percentage\_error: 15.4832
Epoch 20/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7924 -
mean\_absolute\_error: 0.6998 - mean\_squared\_error: 0.7924 -
mean\_absolute\_percentage\_error: 17.1904 - val\_loss: 0.6522 - val\_mean\_absolute\_error:
0.6300 - val\_mean\_squared\_error: 0.6522 - val\_mean\_absolute\_percentage\_error: 15.5306
Epoch 21/100
3608/3608 [==============================] - 0s 89us/step - loss: 0.7939 -
mean\_absolute\_error: 0.6977 - mean\_squared\_error: 0.7939 -
mean\_absolute\_percentage\_error: 17.1475 - val\_loss: 0.6499 - val\_mean\_absolute\_error:
0.6254 - val\_mean\_squared\_error: 0.6499 - val\_mean\_absolute\_percentage\_error: 15.2426
Epoch 22/100
3608/3608 [==============================] - 0s 87us/step - loss: 0.7978 -
mean\_absolute\_error: 0.6966 - mean\_squared\_error: 0.7978 -
mean\_absolute\_percentage\_error: 17.1316 - val\_loss: 0.6505 - val\_mean\_absolute\_error:
0.6262 - val\_mean\_squared\_error: 0.6505 - val\_mean\_absolute\_percentage\_error: 15.3209
Epoch 23/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.7887 -
mean\_absolute\_error: 0.6939 - mean\_squared\_error: 0.7887 -
mean\_absolute\_percentage\_error: 17.0469 - val\_loss: 0.6543 - val\_mean\_absolute\_error:
0.6355 - val\_mean\_squared\_error: 0.6543 - val\_mean\_absolute\_percentage\_error: 15.7981
Epoch 24/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7961 -
mean\_absolute\_error: 0.6989 - mean\_squared\_error: 0.7961 -
mean\_absolute\_percentage\_error: 17.1992 - val\_loss: 0.6656 - val\_mean\_absolute\_error:
0.6258 - val\_mean\_squared\_error: 0.6656 - val\_mean\_absolute\_percentage\_error: 14.8280
Epoch 25/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7868 -
mean\_absolute\_error: 0.6939 - mean\_squared\_error: 0.7868 -
mean\_absolute\_percentage\_error: 17.0604 - val\_loss: 0.6520 - val\_mean\_absolute\_error:
0.6301 - val\_mean\_squared\_error: 0.6520 - val\_mean\_absolute\_percentage\_error: 15.5300
Epoch 26/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7828 -
mean\_absolute\_error: 0.6931 - mean\_squared\_error: 0.7828 -
mean\_absolute\_percentage\_error: 17.0444 - val\_loss: 0.6532 - val\_mean\_absolute\_error:
0.6344 - val\_mean\_squared\_error: 0.6532 - val\_mean\_absolute\_percentage\_error: 15.7533
Epoch 27/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7887 -
mean\_absolute\_error: 0.6944 - mean\_squared\_error: 0.7887 -
mean\_absolute\_percentage\_error: 17.1120 - val\_loss: 0.6496 - val\_mean\_absolute\_error:
0.6275 - val\_mean\_squared\_error: 0.6496 - val\_mean\_absolute\_percentage\_error: 15.4346
Epoch 28/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7848 -
mean\_absolute\_error: 0.6941 - mean\_squared\_error: 0.7848 -
mean\_absolute\_percentage\_error: 17.0886 - val\_loss: 0.6535 - val\_mean\_absolute\_error:
0.6225 - val\_mean\_squared\_error: 0.6535 - val\_mean\_absolute\_percentage\_error: 14.9719
Epoch 29/100
3608/3608 [==============================] - 0s 70us/step - loss: 0.7929 -
mean\_absolute\_error: 0.6963 - mean\_squared\_error: 0.7929 -
mean\_absolute\_percentage\_error: 17.1241 - val\_loss: 0.6536 - val\_mean\_absolute\_error:
0.6237 - val\_mean\_squared\_error: 0.6536 - val\_mean\_absolute\_percentage\_error: 15.0720
Epoch 30/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7850 -
mean\_absolute\_error: 0.6942 - mean\_squared\_error: 0.7850 -
mean\_absolute\_percentage\_error: 17.0982 - val\_loss: 0.6858 - val\_mean\_absolute\_error:
0.6322 - val\_mean\_squared\_error: 0.6858 - val\_mean\_absolute\_percentage\_error: 14.7814
Epoch 31/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7928 -
mean\_absolute\_error: 0.6965 - mean\_squared\_error: 0.7928 -
mean\_absolute\_percentage\_error: 17.0794 - val\_loss: 0.6563 - val\_mean\_absolute\_error:
0.6242 - val\_mean\_squared\_error: 0.6563 - val\_mean\_absolute\_percentage\_error: 14.9820
Epoch 32/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7855 -
mean\_absolute\_error: 0.6957 - mean\_squared\_error: 0.7855 -
mean\_absolute\_percentage\_error: 17.1048 - val\_loss: 0.6588 - val\_mean\_absolute\_error:
0.6238 - val\_mean\_squared\_error: 0.6588 - val\_mean\_absolute\_percentage\_error: 14.9114
Epoch 33/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7864 -
mean\_absolute\_error: 0.6932 - mean\_squared\_error: 0.7864 -
mean\_absolute\_percentage\_error: 17.0517 - val\_loss: 0.6581 - val\_mean\_absolute\_error:
0.6243 - val\_mean\_squared\_error: 0.6581 - val\_mean\_absolute\_percentage\_error: 14.9701
Epoch 34/100
3608/3608 [==============================] - 0s 67us/step - loss: 0.7790 -
mean\_absolute\_error: 0.6894 - mean\_squared\_error: 0.7790 -
mean\_absolute\_percentage\_error: 16.9497 - val\_loss: 0.6516 - val\_mean\_absolute\_error:
0.6246 - val\_mean\_squared\_error: 0.6516 - val\_mean\_absolute\_percentage\_error: 15.1783
Epoch 35/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7795 -
mean\_absolute\_error: 0.6888 - mean\_squared\_error: 0.7795 -
mean\_absolute\_percentage\_error: 16.9439 - val\_loss: 0.6974 - val\_mean\_absolute\_error:
0.6368 - val\_mean\_squared\_error: 0.6974 - val\_mean\_absolute\_percentage\_error: 14.7828
Epoch 36/100
3608/3608 [==============================] - 0s 82us/step - loss: 0.7918 -
mean\_absolute\_error: 0.6970 - mean\_squared\_error: 0.7918 -
mean\_absolute\_percentage\_error: 17.1023 - val\_loss: 0.8574 - val\_mean\_absolute\_error:
0.7602 - val\_mean\_squared\_error: 0.8574 - val\_mean\_absolute\_percentage\_error: 20.0613
Epoch 37/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.7895 -
mean\_absolute\_error: 0.6926 - mean\_squared\_error: 0.7895 -
mean\_absolute\_percentage\_error: 17.0192 - val\_loss: 0.6589 - val\_mean\_absolute\_error:
0.6238 - val\_mean\_squared\_error: 0.6589 - val\_mean\_absolute\_percentage\_error: 14.9419
Epoch 38/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7753 -
mean\_absolute\_error: 0.6872 - mean\_squared\_error: 0.7753 -
mean\_absolute\_percentage\_error: 16.9117 - val\_loss: 0.6619 - val\_mean\_absolute\_error:
0.6242 - val\_mean\_squared\_error: 0.6619 - val\_mean\_absolute\_percentage\_error: 14.8402
Epoch 39/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7737 -
mean\_absolute\_error: 0.6871 - mean\_squared\_error: 0.7737 -
mean\_absolute\_percentage\_error: 16.9143 - val\_loss: 0.6625 - val\_mean\_absolute\_error:
0.6252 - val\_mean\_squared\_error: 0.6625 - val\_mean\_absolute\_percentage\_error: 14.8421
Epoch 40/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7837 -
mean\_absolute\_error: 0.6925 - mean\_squared\_error: 0.7837 -
mean\_absolute\_percentage\_error: 17.0062 - val\_loss: 0.6529 - val\_mean\_absolute\_error:
0.6224 - val\_mean\_squared\_error: 0.6529 - val\_mean\_absolute\_percentage\_error: 14.9709
Epoch 41/100
3608/3608 [==============================] - 0s 65us/step - loss: 0.7825 -
mean\_absolute\_error: 0.6921 - mean\_squared\_error: 0.7825 -
mean\_absolute\_percentage\_error: 16.9949 - val\_loss: 0.7163 - val\_mean\_absolute\_error:
0.6821 - val\_mean\_squared\_error: 0.7163 - val\_mean\_absolute\_percentage\_error: 17.5851
Epoch 42/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7795 -
mean\_absolute\_error: 0.6896 - mean\_squared\_error: 0.7795 -
mean\_absolute\_percentage\_error: 16.9810 - val\_loss: 0.6640 - val\_mean\_absolute\_error:
0.6250 - val\_mean\_squared\_error: 0.6640 - val\_mean\_absolute\_percentage\_error: 14.8126
Epoch 43/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.7766 -
mean\_absolute\_error: 0.6895 - mean\_squared\_error: 0.7766 -
mean\_absolute\_percentage\_error: 16.9793 - val\_loss: 0.6533 - val\_mean\_absolute\_error:
0.6336 - val\_mean\_squared\_error: 0.6533 - val\_mean\_absolute\_percentage\_error: 15.6088
Epoch 44/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7840 -
mean\_absolute\_error: 0.6913 - mean\_squared\_error: 0.7840 -
mean\_absolute\_percentage\_error: 16.9916 - val\_loss: 0.6481 - val\_mean\_absolute\_error:
0.6231 - val\_mean\_squared\_error: 0.6481 - val\_mean\_absolute\_percentage\_error: 15.1227
Epoch 45/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7750 -
mean\_absolute\_error: 0.6882 - mean\_squared\_error: 0.7750 -
mean\_absolute\_percentage\_error: 16.9569 - val\_loss: 0.6516 - val\_mean\_absolute\_error:
0.6222 - val\_mean\_squared\_error: 0.6516 - val\_mean\_absolute\_percentage\_error: 14.9164
Epoch 46/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7788 -
mean\_absolute\_error: 0.6884 - mean\_squared\_error: 0.7788 -
mean\_absolute\_percentage\_error: 16.9091 - val\_loss: 0.6503 - val\_mean\_absolute\_error:
0.6298 - val\_mean\_squared\_error: 0.6503 - val\_mean\_absolute\_percentage\_error: 15.5498
Epoch 47/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7821 -
mean\_absolute\_error: 0.6917 - mean\_squared\_error: 0.7821 -
mean\_absolute\_percentage\_error: 17.0244 - val\_loss: 0.6670 - val\_mean\_absolute\_error:
0.6251 - val\_mean\_squared\_error: 0.6670 - val\_mean\_absolute\_percentage\_error: 14.7334
Epoch 48/100
3608/3608 [==============================] - 0s 76us/step - loss: 0.7722 -
mean\_absolute\_error: 0.6863 - mean\_squared\_error: 0.7722 -
mean\_absolute\_percentage\_error: 16.8488 - val\_loss: 0.6645 - val\_mean\_absolute\_error:
0.6476 - val\_mean\_squared\_error: 0.6645 - val\_mean\_absolute\_percentage\_error: 16.3037
Epoch 49/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7763 -
mean\_absolute\_error: 0.6871 - mean\_squared\_error: 0.7763 -
mean\_absolute\_percentage\_error: 16.8933 - val\_loss: 0.6521 - val\_mean\_absolute\_error:
0.6364 - val\_mean\_squared\_error: 0.6521 - val\_mean\_absolute\_percentage\_error: 15.8662
Epoch 50/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7706 -
mean\_absolute\_error: 0.6852 - mean\_squared\_error: 0.7706 -
mean\_absolute\_percentage\_error: 16.8519 - val\_loss: 0.6577 - val\_mean\_absolute\_error:
0.6412 - val\_mean\_squared\_error: 0.6577 - val\_mean\_absolute\_percentage\_error: 16.0950
Epoch 51/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7762 -
mean\_absolute\_error: 0.6870 - mean\_squared\_error: 0.7762 -
mean\_absolute\_percentage\_error: 16.8881 - val\_loss: 0.7151 - val\_mean\_absolute\_error:
0.6813 - val\_mean\_squared\_error: 0.7151 - val\_mean\_absolute\_percentage\_error: 17.5635
Epoch 52/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7734 -
mean\_absolute\_error: 0.6867 - mean\_squared\_error: 0.7734 -
mean\_absolute\_percentage\_error: 16.9103 - val\_loss: 0.6426 - val\_mean\_absolute\_error:
0.6226 - val\_mean\_squared\_error: 0.6426 - val\_mean\_absolute\_percentage\_error: 15.1597
Epoch 53/100
3608/3608 [==============================] - 0s 67us/step - loss: 0.7894 -
mean\_absolute\_error: 0.6932 - mean\_squared\_error: 0.7894 -
mean\_absolute\_percentage\_error: 17.0412 - val\_loss: 0.7597 - val\_mean\_absolute\_error:
0.7104 - val\_mean\_squared\_error: 0.7597 - val\_mean\_absolute\_percentage\_error: 18.5070
Epoch 54/100
3608/3608 [==============================] - 0s 81us/step - loss: 0.7757 -
mean\_absolute\_error: 0.6880 - mean\_squared\_error: 0.7757 -
mean\_absolute\_percentage\_error: 16.9401 - val\_loss: 0.6464 - val\_mean\_absolute\_error:
0.6204 - val\_mean\_squared\_error: 0.6464 - val\_mean\_absolute\_percentage\_error: 14.8974
Epoch 55/100
3608/3608 [==============================] - 0s 65us/step - loss: 0.7692 -
mean\_absolute\_error: 0.6860 - mean\_squared\_error: 0.7692 -
mean\_absolute\_percentage\_error: 16.9003 - val\_loss: 0.6517 - val\_mean\_absolute\_error:
0.6200 - val\_mean\_squared\_error: 0.6517 - val\_mean\_absolute\_percentage\_error: 14.7674
Epoch 56/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7844 -
mean\_absolute\_error: 0.6940 - mean\_squared\_error: 0.7844 -
mean\_absolute\_percentage\_error: 17.0533 - val\_loss: 0.6580 - val\_mean\_absolute\_error:
0.6444 - val\_mean\_squared\_error: 0.6580 - val\_mean\_absolute\_percentage\_error: 16.2114
Epoch 57/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7725 -
mean\_absolute\_error: 0.6870 - mean\_squared\_error: 0.7725 -
mean\_absolute\_percentage\_error: 16.8932 - val\_loss: 0.6501 - val\_mean\_absolute\_error:
0.6350 - val\_mean\_squared\_error: 0.6501 - val\_mean\_absolute\_percentage\_error: 15.8699
Epoch 58/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7708 -
mean\_absolute\_error: 0.6857 - mean\_squared\_error: 0.7708 -
mean\_absolute\_percentage\_error: 16.8863 - val\_loss: 0.6407 - val\_mean\_absolute\_error:
0.6198 - val\_mean\_squared\_error: 0.6407 - val\_mean\_absolute\_percentage\_error: 15.0010
Epoch 59/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7710 -
mean\_absolute\_error: 0.6880 - mean\_squared\_error: 0.7710 -
mean\_absolute\_percentage\_error: 16.9569 - val\_loss: 0.6475 - val\_mean\_absolute\_error:
0.6195 - val\_mean\_squared\_error: 0.6475 - val\_mean\_absolute\_percentage\_error: 14.7975
Epoch 60/100
3608/3608 [==============================] - 0s 79us/step - loss: 0.7732 -
mean\_absolute\_error: 0.6866 - mean\_squared\_error: 0.7732 -
mean\_absolute\_percentage\_error: 16.8717 - val\_loss: 0.6405 - val\_mean\_absolute\_error:
0.6201 - val\_mean\_squared\_error: 0.6405 - val\_mean\_absolute\_percentage\_error: 15.0508
Epoch 61/100
3608/3608 [==============================] - 0s 70us/step - loss: 0.7789 -
mean\_absolute\_error: 0.6909 - mean\_squared\_error: 0.7789 -
mean\_absolute\_percentage\_error: 17.0159 - val\_loss: 0.6684 - val\_mean\_absolute\_error:
0.6271 - val\_mean\_squared\_error: 0.6684 - val\_mean\_absolute\_percentage\_error: 14.7729
Epoch 62/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7707 -
mean\_absolute\_error: 0.6861 - mean\_squared\_error: 0.7707 -
mean\_absolute\_percentage\_error: 16.8917 - val\_loss: 0.6383 - val\_mean\_absolute\_error:
0.6204 - val\_mean\_squared\_error: 0.6383 - val\_mean\_absolute\_percentage\_error: 15.1429
Epoch 63/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7696 -
mean\_absolute\_error: 0.6856 - mean\_squared\_error: 0.7696 -
mean\_absolute\_percentage\_error: 16.8568 - val\_loss: 0.6389 - val\_mean\_absolute\_error:
0.6253 - val\_mean\_squared\_error: 0.6389 - val\_mean\_absolute\_percentage\_error: 15.3605
Epoch 64/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7683 -
mean\_absolute\_error: 0.6845 - mean\_squared\_error: 0.7683 -
mean\_absolute\_percentage\_error: 16.8670 - val\_loss: 0.6465 - val\_mean\_absolute\_error:
0.6194 - val\_mean\_squared\_error: 0.6465 - val\_mean\_absolute\_percentage\_error: 14.8502
Epoch 65/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7781 -
mean\_absolute\_error: 0.6903 - mean\_squared\_error: 0.7781 -
mean\_absolute\_percentage\_error: 16.9981 - val\_loss: 0.6392 - val\_mean\_absolute\_error:
0.6243 - val\_mean\_squared\_error: 0.6392 - val\_mean\_absolute\_percentage\_error: 15.2604
Epoch 66/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7759 -
mean\_absolute\_error: 0.6882 - mean\_squared\_error: 0.7759 -
mean\_absolute\_percentage\_error: 16.9158 - val\_loss: 0.6410 - val\_mean\_absolute\_error:
0.6187 - val\_mean\_squared\_error: 0.6410 - val\_mean\_absolute\_percentage\_error: 14.9516
Epoch 67/100
3608/3608 [==============================] - 0s 80us/step - loss: 0.7688 -
mean\_absolute\_error: 0.6831 - mean\_squared\_error: 0.7688 -
mean\_absolute\_percentage\_error: 16.8240 - val\_loss: 0.6426 - val\_mean\_absolute\_error:
0.6186 - val\_mean\_squared\_error: 0.6426 - val\_mean\_absolute\_percentage\_error: 14.8280
Epoch 68/100
3608/3608 [==============================] - 0s 83us/step - loss: 0.7764 -
mean\_absolute\_error: 0.6872 - mean\_squared\_error: 0.7764 -
mean\_absolute\_percentage\_error: 16.9149 - val\_loss: 0.6994 - val\_mean\_absolute\_error:
0.6366 - val\_mean\_squared\_error: 0.6994 - val\_mean\_absolute\_percentage\_error: 14.6619
Epoch 69/100
3608/3608 [==============================] - 0s 78us/step - loss: 0.7780 -
mean\_absolute\_error: 0.6927 - mean\_squared\_error: 0.7780 -
mean\_absolute\_percentage\_error: 17.0179 - val\_loss: 0.6537 - val\_mean\_absolute\_error:
0.6436 - val\_mean\_squared\_error: 0.6537 - val\_mean\_absolute\_percentage\_error: 16.1619
Epoch 70/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7688 -
mean\_absolute\_error: 0.6851 - mean\_squared\_error: 0.7688 -
mean\_absolute\_percentage\_error: 16.8999 - val\_loss: 0.6378 - val\_mean\_absolute\_error:
0.6270 - val\_mean\_squared\_error: 0.6378 - val\_mean\_absolute\_percentage\_error: 15.5348
Epoch 71/100
3608/3608 [==============================] - 0s 79us/step - loss: 0.7718 -
mean\_absolute\_error: 0.6841 - mean\_squared\_error: 0.7718 -
mean\_absolute\_percentage\_error: 16.8377 - val\_loss: 0.6344 - val\_mean\_absolute\_error:
0.6231 - val\_mean\_squared\_error: 0.6344 - val\_mean\_absolute\_percentage\_error: 15.3256
Epoch 72/100
3608/3608 [==============================] - 0s 79us/step - loss: 0.7743 -
mean\_absolute\_error: 0.6862 - mean\_squared\_error: 0.7743 -
mean\_absolute\_percentage\_error: 16.8985 - val\_loss: 0.6613 - val\_mean\_absolute\_error:
0.6230 - val\_mean\_squared\_error: 0.6613 - val\_mean\_absolute\_percentage\_error: 14.6635
Epoch 73/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7636 -
mean\_absolute\_error: 0.6832 - mean\_squared\_error: 0.7636 -
mean\_absolute\_percentage\_error: 16.8008 - val\_loss: 0.6348 - val\_mean\_absolute\_error:
0.6262 - val\_mean\_squared\_error: 0.6348 - val\_mean\_absolute\_percentage\_error: 15.4772
Epoch 74/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7739 -
mean\_absolute\_error: 0.6866 - mean\_squared\_error: 0.7739 -
mean\_absolute\_percentage\_error: 16.8875 - val\_loss: 0.6484 - val\_mean\_absolute\_error:
0.6400 - val\_mean\_squared\_error: 0.6484 - val\_mean\_absolute\_percentage\_error: 16.0957
Epoch 75/100
3608/3608 [==============================] - 0s 83us/step - loss: 0.7703 -
mean\_absolute\_error: 0.6856 - mean\_squared\_error: 0.7703 -
mean\_absolute\_percentage\_error: 16.9059 - val\_loss: 0.6407 - val\_mean\_absolute\_error:
0.6168 - val\_mean\_squared\_error: 0.6407 - val\_mean\_absolute\_percentage\_error: 14.7856
Epoch 76/100
3608/3608 [==============================] - 0s 65us/step - loss: 0.7710 -
mean\_absolute\_error: 0.6856 - mean\_squared\_error: 0.7710 -
mean\_absolute\_percentage\_error: 16.8540 - val\_loss: 0.6443 - val\_mean\_absolute\_error:
0.6169 - val\_mean\_squared\_error: 0.6443 - val\_mean\_absolute\_percentage\_error: 14.6883
Epoch 77/100
3608/3608 [==============================] - 0s 66us/step - loss: 0.7731 -
mean\_absolute\_error: 0.6862 - mean\_squared\_error: 0.7731 -
mean\_absolute\_percentage\_error: 16.8779 - val\_loss: 0.6351 - val\_mean\_absolute\_error:
0.6220 - val\_mean\_squared\_error: 0.6351 - val\_mean\_absolute\_percentage\_error: 15.2425
Epoch 78/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7659 -
mean\_absolute\_error: 0.6827 - mean\_squared\_error: 0.7659 -
mean\_absolute\_percentage\_error: 16.7972 - val\_loss: 0.6578 - val\_mean\_absolute\_error:
0.6477 - val\_mean\_squared\_error: 0.6578 - val\_mean\_absolute\_percentage\_error: 16.4076
Epoch 79/100
3608/3608 [==============================] - 0s 67us/step - loss: 0.7786 -
mean\_absolute\_error: 0.6899 - mean\_squared\_error: 0.7786 -
mean\_absolute\_percentage\_error: 16.9437 - val\_loss: 0.6318 - val\_mean\_absolute\_error:
0.6172 - val\_mean\_squared\_error: 0.6318 - val\_mean\_absolute\_percentage\_error: 15.0224
Epoch 80/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7640 -
mean\_absolute\_error: 0.6824 - mean\_squared\_error: 0.7640 -
mean\_absolute\_percentage\_error: 16.7999 - val\_loss: 0.6392 - val\_mean\_absolute\_error:
0.6326 - val\_mean\_squared\_error: 0.6392 - val\_mean\_absolute\_percentage\_error: 15.8111
Epoch 81/100
3608/3608 [==============================] - 0s 68us/step - loss: 0.7731 -
mean\_absolute\_error: 0.6879 - mean\_squared\_error: 0.7731 -
mean\_absolute\_percentage\_error: 16.9616 - val\_loss: 0.6426 - val\_mean\_absolute\_error:
0.6165 - val\_mean\_squared\_error: 0.6426 - val\_mean\_absolute\_percentage\_error: 14.7122
Epoch 82/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7638 -
mean\_absolute\_error: 0.6836 - mean\_squared\_error: 0.7638 -
mean\_absolute\_percentage\_error: 16.8187 - val\_loss: 0.6490 - val\_mean\_absolute\_error:
0.6387 - val\_mean\_squared\_error: 0.6490 - val\_mean\_absolute\_percentage\_error: 16.0842
Epoch 83/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7649 -
mean\_absolute\_error: 0.6832 - mean\_squared\_error: 0.7649 -
mean\_absolute\_percentage\_error: 16.8338 - val\_loss: 0.6365 - val\_mean\_absolute\_error:
0.6231 - val\_mean\_squared\_error: 0.6365 - val\_mean\_absolute\_percentage\_error: 15.3364
Epoch 84/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7610 -
mean\_absolute\_error: 0.6808 - mean\_squared\_error: 0.7610 -
mean\_absolute\_percentage\_error: 16.7460 - val\_loss: 0.6374 - val\_mean\_absolute\_error:
0.6309 - val\_mean\_squared\_error: 0.6374 - val\_mean\_absolute\_percentage\_error: 15.7225
Epoch 85/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7632 -
mean\_absolute\_error: 0.6820 - mean\_squared\_error: 0.7632 -
mean\_absolute\_percentage\_error: 16.8080 - val\_loss: 0.6330 - val\_mean\_absolute\_error:
0.6169 - val\_mean\_squared\_error: 0.6330 - val\_mean\_absolute\_percentage\_error: 14.9450
Epoch 86/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7618 -
mean\_absolute\_error: 0.6808 - mean\_squared\_error: 0.7618 -
mean\_absolute\_percentage\_error: 16.7490 - val\_loss: 0.6585 - val\_mean\_absolute\_error:
0.6491 - val\_mean\_squared\_error: 0.6585 - val\_mean\_absolute\_percentage\_error: 16.3947
Epoch 87/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7623 -
mean\_absolute\_error: 0.6819 - mean\_squared\_error: 0.7623 -
mean\_absolute\_percentage\_error: 16.7731 - val\_loss: 0.6319 - val\_mean\_absolute\_error:
0.6179 - val\_mean\_squared\_error: 0.6319 - val\_mean\_absolute\_percentage\_error: 15.0404
Epoch 88/100
3608/3608 [==============================] - 0s 74us/step - loss: 0.7614 -
mean\_absolute\_error: 0.6802 - mean\_squared\_error: 0.7614 -
mean\_absolute\_percentage\_error: 16.7213 - val\_loss: 0.6362 - val\_mean\_absolute\_error:
0.6153 - val\_mean\_squared\_error: 0.6362 - val\_mean\_absolute\_percentage\_error: 14.8482
Epoch 89/100
3608/3608 [==============================] - 0s 69us/step - loss: 0.7691 -
mean\_absolute\_error: 0.6827 - mean\_squared\_error: 0.7691 -
mean\_absolute\_percentage\_error: 16.8109 - val\_loss: 0.6551 - val\_mean\_absolute\_error:
0.6195 - val\_mean\_squared\_error: 0.6551 - val\_mean\_absolute\_percentage\_error: 14.5960
Epoch 90/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7608 -
mean\_absolute\_error: 0.6806 - mean\_squared\_error: 0.7608 -
mean\_absolute\_percentage\_error: 16.7607 - val\_loss: 0.6406 - val\_mean\_absolute\_error:
0.6337 - val\_mean\_squared\_error: 0.6406 - val\_mean\_absolute\_percentage\_error: 15.8574
Epoch 91/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7613 -
mean\_absolute\_error: 0.6829 - mean\_squared\_error: 0.7613 -
mean\_absolute\_percentage\_error: 16.8414 - val\_loss: 0.6856 - val\_mean\_absolute\_error:
0.6300 - val\_mean\_squared\_error: 0.6856 - val\_mean\_absolute\_percentage\_error: 14.5480
Epoch 92/100
3608/3608 [==============================] - 0s 69us/step - loss: 0.7664 -
mean\_absolute\_error: 0.6845 - mean\_squared\_error: 0.7664 -
mean\_absolute\_percentage\_error: 16.7957 - val\_loss: 0.6483 - val\_mean\_absolute\_error:
0.6414 - val\_mean\_squared\_error: 0.6483 - val\_mean\_absolute\_percentage\_error: 16.1801
Epoch 93/100
3608/3608 [==============================] - 0s 87us/step - loss: 0.7678 -
mean\_absolute\_error: 0.6830 - mean\_squared\_error: 0.7678 -
mean\_absolute\_percentage\_error: 16.8405 - val\_loss: 0.6421 - val\_mean\_absolute\_error:
0.6160 - val\_mean\_squared\_error: 0.6421 - val\_mean\_absolute\_percentage\_error: 14.7362
Epoch 94/100
3608/3608 [==============================] - 0s 71us/step - loss: 0.7614 -
mean\_absolute\_error: 0.6818 - mean\_squared\_error: 0.7614 -
mean\_absolute\_percentage\_error: 16.7564 - val\_loss: 0.6284 - val\_mean\_absolute\_error:
0.6162 - val\_mean\_squared\_error: 0.6284 - val\_mean\_absolute\_percentage\_error: 15.0525
Epoch 95/100
3608/3608 [==============================] - 0s 72us/step - loss: 0.7615 -
mean\_absolute\_error: 0.6803 - mean\_squared\_error: 0.7615 -
mean\_absolute\_percentage\_error: 16.7422 - val\_loss: 0.6354 - val\_mean\_absolute\_error:
0.6176 - val\_mean\_squared\_error: 0.6354 - val\_mean\_absolute\_percentage\_error: 14.8683
Epoch 96/100
3608/3608 [==============================] - 0s 77us/step - loss: 0.7641 -
mean\_absolute\_error: 0.6809 - mean\_squared\_error: 0.7641 -
mean\_absolute\_percentage\_error: 16.7629 - val\_loss: 0.6596 - val\_mean\_absolute\_error:
0.6213 - val\_mean\_squared\_error: 0.6596 - val\_mean\_absolute\_percentage\_error: 14.6465
Epoch 97/100
3608/3608 [==============================] - 0s 75us/step - loss: 0.7621 -
mean\_absolute\_error: 0.6788 - mean\_squared\_error: 0.7621 -
mean\_absolute\_percentage\_error: 16.6708 - val\_loss: 0.6295 - val\_mean\_absolute\_error:
0.6181 - val\_mean\_squared\_error: 0.6295 - val\_mean\_absolute\_percentage\_error: 15.1627
Epoch 98/100
3608/3608 [==============================] - 0s 73us/step - loss: 0.7604 -
mean\_absolute\_error: 0.6807 - mean\_squared\_error: 0.7604 -
mean\_absolute\_percentage\_error: 16.7753 - val\_loss: 0.6730 - val\_mean\_absolute\_error:
0.6254 - val\_mean\_squared\_error: 0.6730 - val\_mean\_absolute\_percentage\_error: 14.5506
Epoch 99/100
3608/3608 [==============================] - 0s 78us/step - loss: 0.7593 -
mean\_absolute\_error: 0.6787 - mean\_squared\_error: 0.7593 -
mean\_absolute\_percentage\_error: 16.7135 - val\_loss: 0.6411 - val\_mean\_absolute\_error:
0.6189 - val\_mean\_squared\_error: 0.6411 - val\_mean\_absolute\_percentage\_error: 14.9029
Epoch 100/100
3608/3608 [==============================] - 0s 80us/step - loss: 0.7615 -
mean\_absolute\_error: 0.6810 - mean\_squared\_error: 0.7615 -
mean\_absolute\_percentage\_error: 16.7354 - val\_loss: 0.6416 - val\_mean\_absolute\_error:
0.6348 - val\_mean\_squared\_error: 0.6416 - val\_mean\_absolute\_percentage\_error: 15.8757

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}181}]:} <keras.callbacks.History at 0x24bee33710>
\end{Verbatim}
            
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}182}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,}\PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nn\PYZus{}model}\PY{o}{.}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nn\PYZus{}model}\PY{o}{.}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural Network Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epochs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_50_0.png}
    \end{center}

    
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}183}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{r2\PYZus{}score}\PY{p}{,} \PY{n}{explained\PYZus{}variance\PYZus{}score}\PY{p}{,} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{,}
          \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{,} \PY{n}{median\PYZus{}absolute\PYZus{}error}
          
          \PY{k}{def} \PY{n+nf}{predict\PYZus{}score}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{:}
          
              \PY{n}{predictions} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
          
              \PY{n}{r2} \PY{o}{=} \PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{expl\PYZus{}var} \PY{o}{=} \PY{n}{explained\PYZus{}variance\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{mse} \PY{o}{=} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{mae} \PY{o}{=} \PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{mede} \PY{o}{=} \PY{n}{median\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{scores} \PY{o}{=} \PY{p}{[}\PY{n}{r2}\PY{p}{,} \PY{n}{expl\PYZus{}var}\PY{p}{,} \PY{n}{mse}\PY{p}{,} \PY{n}{mae}\PY{p}{,} \PY{n}{mede}\PY{p}{]}
          
              \PY{n}{regressor} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{predictions}\PY{p}{)}
              \PY{n}{regr\PYZus{}line} \PY{o}{=} \PY{n}{regressor}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}
          
              \PY{k}{return} \PY{n}{predictions}\PY{p}{,} \PY{n}{scores}\PY{p}{,} \PY{n}{regr\PYZus{}line}
          
          \PY{n}{lin\PYZus{}pred}\PY{p}{,} \PY{n}{lin\PYZus{}score}\PY{p}{,} \PY{n}{lin\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{linear\PYZus{}reg}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{)}
          \PY{n}{ridge\PYZus{}pred}\PY{p}{,} \PY{n}{ridge\PYZus{}score}\PY{p}{,} \PY{n}{ridge\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{ridge\PYZus{}reg}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{)}
          \PY{n}{lasso\PYZus{}pred}\PY{p}{,} \PY{n}{lasso\PYZus{}score}\PY{p}{,} \PY{n}{lasso\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{lasso\PYZus{}reg}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{)}
          \PY{n}{nn\PYZus{}pred}\PY{p}{,} \PY{n}{nn\PYZus{}score}\PY{p}{,} \PY{n}{nn\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{nn\PYZus{}model}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}184}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,}
          \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted vs Observed Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.04}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Observed Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ha}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mf}{0.04}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{va}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vertical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear regression predictions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{,} \PY{n}{lin\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{lin\PYZus{}pred}\PY{p}{,} \PY{n}{lin\PYZus{}line}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear regression fit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ridge regression predictions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{,} \PY{n}{ridge\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ridge regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{ridge\PYZus{}pred}\PY{p}{,} \PY{n}{ridge\PYZus{}line}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ridge regression fit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lasso regression predictions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{,} \PY{n}{lasso\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lasso regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{lasso\PYZus{}pred}\PY{p}{,} \PY{n}{lasso\PYZus{}line}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lasso regression fit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Neural Network predictions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}log\PYZus{}test}\PY{p}{,} \PY{n}{nn\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neural network}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{nn\PYZus{}pred}\PY{p}{,} \PY{n}{nn\PYZus{}line}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neural network fit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}184}]:} <matplotlib.legend.Legend at 0x24bf2de5c0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_52_1.png}
    \end{center}
    
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}185}]:} \PY{k}{def} \PY{n+nf}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{n}{index}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{p}{[}\PY{n}{lin\PYZus{}score}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{n}{ridge\PYZus{}score}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{n}{lasso\PYZus{}score}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{,} \PY{n}{nn\PYZus{}score}\PY{p}{[}\PY{n}{index}\PY{p}{]}\PY{p}{]}
          
          \PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}
                      \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Algorithm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ridge Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Lasso}
          \PY{n}{Regression}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{,}\PY{l+s+s2}{\PYZdq{}}\PY{n}{Neural} \PY{n}{Network}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{],}
                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{r2 score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}
                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Explained variance score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{n}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                       \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Squared Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mean Absolute Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                       \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Median Absolute Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{create\PYZus{}table\PYZus{}row}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
          
          \PY{n}{scores\PYZus{}df}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/artist-model-scores.png}
    \end{center}
            
    \subsubsection{Summary}

From the coefficients of the linear regression models, we see that the
number of page views and watchers has the largest positive coefficients
(0.24 and 0.22 respectively); this suggests that if the artist
previously has a huge following and attracts a lot of visitors to their
account page, they probably are a popular artist whose art is well-liked
(and so get a high number of favorites on their image). The number of
deviations has the largest negative coefficient (-0.37); this suggests
that artists with a high number of posts tend to have less favorites on
their posts - perhaps because it is quicker to produce many pieces of
lower-quality work vs high-quality work that receives more favorites but
also takes more time to make.\\

The other predictors have rather weak coefficient values within the
range of {[}-0.05, 0.05{]}.\\

From the graphs and the scoring dataframe, we see that all the models
have roughly the same scores. This means that given a particular image's
log-normalized artist attributes, the models will predict the resulting
number of favorites with about a 69\% - 70\% error rate. The explained
variance scores of roughly 0.32 suggests that given the log-normalized
social metrics, the model is able to account for roughly 32\% of the
variation within the data. Since the neural network is not outperforming
the linear regression models, we can assume that the data has an
underlying linear relationship that can be sufficiently modeled by
linear models.\\

Overall, I would say that models perform relatively well. The prediction
vs observed values graphs reveal that the predictions at least tend to
be within the same neighborhood of the observed values. However, it also
appears that the predictions are more accurate when the observed number
of likes for a given image are around the average range (3 - 5 number of
favorites on the log-normalized scale), while more popular images (7 - 9
on the scale) tend to receive less accurate predictions.\\

Since the performance of the linear models and the neural network model
are quite similar, I would ultimately choose the linear models for
prediction of image favorites based based on artist attributes since the
linear models have a simpler setup in terms of code and training time,
and have more interpretable results since they represent less complex
functions.
\newpage
    \subsubsection{Unsupervised Learning - K-Means Clustering}

As a fun experiment, I decided to use K-Means clustering to see whether
there were any discernible clusters that could be formed from the artist
attributes.\\

K-means clustering is an unsupervised learning method commonly used to
identify clusters within a dataset. The algorithm works by randomly
selecting cluster centers (known as centroids), assigning each data
point to their nearest centroid by choosing the one with the smallest
Euclidean distance, and then updating the centroid value based on the
mean of the data points. The process repeats until a convergence is
obtained, i.e. when the centroid values no longer change, thus
indicating that "stable" clusters have been found.

\subsubsection*{Choosing Optimal K using Elbow Plot}

To choose an appropriate number of clusters (K), I tested out the
K-means algorithm with K values in the range of 1 to 15 and plotted an
elbow plot of sum of squared distances within clusters, i.e. the total
variation within each cluster, vs the number of clusters.\\

The elbow plot is a useful way to determine the optimal K value to
choose since it allows us to observe the point at which the
within-cluster sum of squares (inertia) starts slowing down in its
decrease. While we aim to have clusters with low inertia, when
increasing K no longer decreases the inertia significantly, that
indicates that the optimal cluster value - the K amount of clusters that
contributed to the most inertia decrease - has already been found.\\

The plot shows a dip in the distances around a K value of 3, and so I
clustered the data into 3 clusters.
\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}188}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{import} \PY{n}{KMeans}
          
          \PY{n}{sum\PYZus{}of\PYZus{}squared\PYZus{}distances} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          
          \PY{n}{K} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)}
          \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n}{K}\PY{p}{:}
              \PY{n}{km} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{n}{k}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}log}\PY{p}{)}
              \PY{n}{sum\PYZus{}of\PYZus{}squared\PYZus{}distances}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{km}\PY{o}{.}\PY{n}{inertia\PYZus{}}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}189}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{120}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{K}\PY{p}{,} \PY{n}{sum\PYZus{}of\PYZus{}squared\PYZus{}distances}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Elbow plot of Sum of Squared Distance vs }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Number of Clusters}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of Clusters (K)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sum of squared distance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          
          \PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}log}\PY{p}{)}
          \PY{n}{cluster\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{kmeans}\PY{o}{.}\PY{n}{cluster\PYZus{}centers\PYZus{}}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{cluster\PYZus{}df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n}{predictors}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{c+c1}{\PYZsh{}remove \PYZsq{}faves\PYZsq{}}
          \PY{n}{cluster\PYZus{}df}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{level}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          \PY{n}{cluster\PYZus{}df}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_57_0.png}
    \end{center}
    
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/k-means.png}
    \end{center}
            \newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}236}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{pd}\PY{o}{.}\PY{n}{plotting}\PY{o}{.}\PY{n}{parallel\PYZus{}coordinates}\PY{p}{(}\PY{n}{cluster\PYZus{}df}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{index}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{colormap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Parallel Coordinates Plot of Artist Clusters}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}236}]:} Text(0.5,1,'Parallel Coordinates Plot of Artist Clusters')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_58_1.png}
    \end{center}

    
    \subsubsection*{Parallel Coordinates Plot}

The parallel coordinates plot shows that the 3 different artist groups
identified appear to differ in the number of comments, critiques,
deviations, faves, forum posts, page views, scraps, and followers in a
rather holistic manner. That is, there are 3 distinct groups separated
by the number of attributes that they have in each domain, with one
group having the highest count in all attributes (group 0), one group
having the lowest count in all attributes (group 2), and one group
having a moderate count in all attributes (group 1). This suggests that
having a high value in each domain is complementary to accumulating
popularity via increased numbers of watchers and faves.
\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}262}]:} \PY{c+c1}{\PYZsh{} assign clusters to each data point using the kmeans model and view pairplot}
          \PY{n}{cluster\PYZus{}preds} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{kmeans}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}log}\PY{p}{)}\PY{p}{)}
          \PY{n}{X\PYZus{}log}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clusters}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{cluster\PYZus{}preds}
          
          \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{X\PYZus{}log}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clusters}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}262}]:} <seaborn.axisgrid.PairGrid at 0x24c879db00>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/DA-analysis_60_1.png}
    \end{center}

    
    \subsubsection*{Pairplot of Cluster Distributions across the Columns}

The pairplot confirms the analysis obtained from the parallel
coordinates plot, i.e. there are 3 distinct artist groups that have low,
medium, and high values across all domains. This makes sense when we
consider the correlations between the artist attributes as shown
previously in the heatmap.
\newpage
    \subsection{Predicting number of favorites based on image features}

As explained earlier, an adapted version of the VGG16 convolutional
neural network is used to predict the number of image favorites based on
image features. The VGG16 network is preset with 'ImageNet' weights, and
the weights are frozen so that they do not have to be retrained, thus
saving computational power, though at the cost of less accuracy. The
original fully-connected layers are replaced with custom fully-connected
layers, with an output layer consisting of a linear activation function
with one output so that a regression prediction can be implemented.\\

After freezing the VGG16 layers, the total trainable weights in the
model is around 2.5 million. The model is compiled with mean squared
error as the loss function and an adam optimizer.\\

The images are resized to a dimension of 250 x 250 x 3 for the adapted
VGG16 model input, and the data is split with a test-set ratio of 0.3,
thus giving us 3946 training data points and 1692 testing data points.

\subsubsection{Training the model}

Due to time constraints, the model is trained for only 50 epochs with a
batch size of 30 and a validation split of 0.2. Due to the small size of
the training data and the data's inherent inconsistency (the variation
between one artwork and the next is significant), it is expected that
the model will probably overfit.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{from} \PY{n+nn}{skimage}\PY{n+nn}{.}\PY{n+nn}{transform} \PY{k}{import} \PY{n}{resize}
         
         \PY{c+c1}{\PYZsh{}create empty image lists}
         \PY{n}{image\PYZus{}resized\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{image\PYZus{}original\PYZus{}lst} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{}height and width of resized image}
         \PY{n}{new\PYZus{}height} \PY{o}{=} \PY{l+m+mi}{250}
         \PY{n}{new\PYZus{}width} \PY{o}{=} \PY{l+m+mi}{250}
         
         \PY{c+c1}{\PYZsh{}image loading has to be done one\PYZhy{}by\PYZhy{}one to ensure that the images are loaded in the}
         \PY{n}{corresponding} \PY{n}{order} \PY{n}{to} \PY{n}{the}
         \PY{c+c1}{\PYZsh{}data collected in the dataframe}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{final\PYZus{}df}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{my\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DA\PYZhy{}images\PYZhy{}2/}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{img\PYZus{}path} \PY{o}{=} \PY{n}{final\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{c+c1}{\PYZsh{}load image}
             \PY{n}{img} \PY{o}{=} \PY{n}{mpimg}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{my\PYZus{}dir} \PY{o}{+} \PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{image\PYZus{}original\PYZus{}lst}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{img}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}resize image}
             \PY{n}{img\PYZus{}resized} \PY{o}{=} \PY{n}{resize}\PY{p}{(}\PY{n}{img}\PY{p}{,}\PY{p}{(}\PY{n}{new\PYZus{}height}\PY{p}{,}\PY{n}{new\PYZus{}width}\PY{p}{)}\PY{p}{,}
                                 \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{constant}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{anti\PYZus{}aliasing}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{anti\PYZus{}aliasing\PYZus{}sigma}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
             \PY{n}{image\PYZus{}resized\PYZus{}lst}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{img\PYZus{}resized}\PY{p}{)}
         
         \PY{n}{original\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{image\PYZus{}original\PYZus{}lst}\PY{p}{)}
         \PY{n}{resized\PYZus{}arr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{image\PYZus{}resized\PYZus{}lst}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Original images array shape: }\PY{l+s+si}{\PYZob{}original\PYZus{}arr.shape\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Resized images array shape:}
         \PY{p}{\PYZob{}}\PY{n}{resized\PYZus{}arr}\PY{o}{.}\PY{n}{shape}\PY{p}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Original images array shape: (5638,)
Resized images array shape: (5638, 250, 250, 3)

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{}peak at the resized images vs original images}
         \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{120}\PY{p}{,}
                                \PY{n}{subplot\PYZus{}kw}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xticks}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yticks}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
                                \PY{n}{gridspec\PYZus{}kw}\PY{o}{=}\PY{n+nb}{dict}\PY{p}{(}\PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{wspace}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{:}
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{original\PYZus{}arr}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{resized\PYZus{}arr}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Faves: }\PY{l+s+si}{\PYZob{}final\PYZus{}df.faves[i]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
         
         \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Images Original}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
         \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Images Resized}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} Text(0,0.5,'Images Resized')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_65_1.png}
    \end{center}
    
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{vgg16} \PY{k}{import} \PY{n}{VGG16}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{vgg16} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}
         
         \PY{c+c1}{\PYZsh{}Get back the convolutional part of a VGG network trained on ImageNet}
         \PY{n}{vgg16} \PY{o}{=} \PY{n}{VGG16}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                       \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{250}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                       \PY{n}{include\PYZus{}top} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
         \PY{n}{vgg16}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}freeze the lower level layers}
         \PY{n}{vgg16}\PY{o}{.}\PY{n}{trainable} \PY{o}{=} \PY{k+kc}{False}
         
         \PY{c+c1}{\PYZsh{}Add the fully\PYZhy{}connected layers}
         \PY{n}{vgg16\PYZus{}adapted} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{vgg16}\PY{p}{)}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{flatten}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fc1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fc2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}In the summary, weights and layers from VGG part will be hidden, but they will be fit}
         \PY{n}{during} \PY{n}{the} \PY{n}{training}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}compile model}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mae}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mape}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#
=================================================================
input\_1 (InputLayer)         (None, 250, 250, 3)       0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block1\_conv1 (Conv2D)        (None, 250, 250, 64)      1792
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block1\_conv2 (Conv2D)        (None, 250, 250, 64)      36928
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block1\_pool (MaxPooling2D)   (None, 125, 125, 64)      0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block2\_conv1 (Conv2D)        (None, 125, 125, 128)     73856
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block2\_conv2 (Conv2D)        (None, 125, 125, 128)     147584
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block2\_pool (MaxPooling2D)   (None, 62, 62, 128)       0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block3\_conv1 (Conv2D)        (None, 62, 62, 256)       295168
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block3\_conv2 (Conv2D)        (None, 62, 62, 256)       590080
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block3\_conv3 (Conv2D)        (None, 62, 62, 256)       590080
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block3\_pool (MaxPooling2D)   (None, 31, 31, 256)       0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block4\_conv1 (Conv2D)        (None, 31, 31, 512)       1180160
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block4\_conv2 (Conv2D)        (None, 31, 31, 512)       2359808
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block4\_conv3 (Conv2D)        (None, 31, 31, 512)       2359808
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block4\_pool (MaxPooling2D)   (None, 15, 15, 512)       0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block5\_conv1 (Conv2D)        (None, 15, 15, 512)       2359808
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block5\_conv2 (Conv2D)        (None, 15, 15, 512)       2359808
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block5\_conv3 (Conv2D)        (None, 15, 15, 512)       2359808
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
block5\_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#
=================================================================
vgg16 (Model)                (None, 7, 7, 512)         14714688
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten (Flatten)            (None, 25088)             0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
fc1 (Dense)                  (None, 100)               2508900
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
fc2 (Dense)                  (None, 32)                3232
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_5 (Dense)              (None, 1)                 33
=================================================================
Total params: 17,226,853
Trainable params: 2,512,165
Non-trainable params: 14,714,688
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{cnn\PYZus{}X} \PY{o}{=} \PY{n}{resized\PYZus{}arr}
         \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{log\PYZus{}predictors\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{cnn\PYZus{}X\PYZus{}train}\PY{p}{,} \PY{n}{cnn\PYZus{}X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{cnn\PYZus{}X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{cnn\PYZus{}X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,}\PY{n}{cnn\PYZus{}X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
(3946, 250, 250, 3) (1692, 250, 250, 3) (3946,) (1692,)

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}42}]:} \PY{c+c1}{\PYZsh{}training the adapted vgg16 model}
         \PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{cnn\PYZus{}X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,}
                           \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,}
                           \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,}
                           \PY{n}{validation\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                           \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Train on 3156 samples, validate on 790 samples
Epoch 1/50
3156/3156 [==============================] - 1531s 485ms/step - loss: 2.8535 -
mean\_squared\_error: 2.8535 - mean\_absolute\_error: 1.1057 -
mean\_absolute\_percentage\_error: 27.0137 - val\_loss: 1.0406 - val\_mean\_squared\_error:
1.0406 - val\_mean\_absolute\_error: 0.8087 - val\_mean\_absolute\_percentage\_error: 19.9656
Epoch 2/50
3156/3156 [==============================] - 1440s 456ms/step - loss: 0.8879 -
mean\_squared\_error: 0.8879 - mean\_absolute\_error: 0.7348 -
mean\_absolute\_percentage\_error: 17.9010 - val\_loss: 1.0832 - val\_mean\_squared\_error:
1.0832 - val\_mean\_absolute\_error: 0.7914 - val\_mean\_absolute\_percentage\_error: 18.4118
Epoch 3/50
3156/3156 [==============================] - 1450s 460ms/step - loss: 0.6372 -
mean\_squared\_error: 0.6372 - mean\_absolute\_error: 0.6173 -
mean\_absolute\_percentage\_error: 15.0267 - val\_loss: 1.4883 - val\_mean\_squared\_error:
1.4883 - val\_mean\_absolute\_error: 1.0268 - val\_mean\_absolute\_percentage\_error: 27.4544
Epoch 4/50
3156/3156 [==============================] - 1488s 471ms/step - loss: 0.5004 -
mean\_squared\_error: 0.5004 - mean\_absolute\_error: 0.5545 -
mean\_absolute\_percentage\_error: 13.5420 - val\_loss: 1.1679 - val\_mean\_squared\_error:
1.1679 - val\_mean\_absolute\_error: 0.8418 - val\_mean\_absolute\_percentage\_error: 20.1998
Epoch 5/50
3156/3156 [==============================] - 1447s 458ms/step - loss: 0.3206 -
mean\_squared\_error: 0.3206 - mean\_absolute\_error: 0.4386 -
mean\_absolute\_percentage\_error: 10.7223 - val\_loss: 1.2091 - val\_mean\_squared\_error:
1.2091 - val\_mean\_absolute\_error: 0.8746 - val\_mean\_absolute\_percentage\_error: 21.7492
Epoch 6/50
3156/3156 [==============================] - 1417s 449ms/step - loss: 0.2263 -
mean\_squared\_error: 0.2263 - mean\_absolute\_error: 0.3671 -
mean\_absolute\_percentage\_error: 9.0110 - val\_loss: 1.4216 - val\_mean\_squared\_error:
1.4216 - val\_mean\_absolute\_error: 0.9604 - val\_mean\_absolute\_percentage\_error: 24.6864
Epoch 7/50
3156/3156 [==============================] - 1335s 423ms/step - loss: 0.1903 -
mean\_squared\_error: 0.1903 - mean\_absolute\_error: 0.3392 -
mean\_absolute\_percentage\_error: 8.3342 - val\_loss: 1.3693 - val\_mean\_squared\_error:
1.3693 - val\_mean\_absolute\_error: 0.9038 - val\_mean\_absolute\_percentage\_error: 21.1029
Epoch 8/50
3156/3156 [==============================] - 1386s 439ms/step - loss: 0.2013 -
mean\_squared\_error: 0.2013 - mean\_absolute\_error: 0.3540 -
mean\_absolute\_percentage\_error: 8.6425 - val\_loss: 1.2561 - val\_mean\_squared\_error:
1.2561 - val\_mean\_absolute\_error: 0.8854 - val\_mean\_absolute\_percentage\_error: 21.8220
Epoch 9/50
3156/3156 [==============================] - 1519s 481ms/step - loss: 0.1588 -
mean\_squared\_error: 0.1588 - mean\_absolute\_error: 0.3126 -
mean\_absolute\_percentage\_error: 7.6218 - val\_loss: 1.2844 - val\_mean\_squared\_error:
1.2844 - val\_mean\_absolute\_error: 0.8988 - val\_mean\_absolute\_percentage\_error: 22.3626
Epoch 10/50
3156/3156 [==============================] - 1530s 485ms/step - loss: 0.1194 -
mean\_squared\_error: 0.1194 - mean\_absolute\_error: 0.2633 -
mean\_absolute\_percentage\_error: 6.4775 - val\_loss: 1.3049 - val\_mean\_squared\_error:
1.3049 - val\_mean\_absolute\_error: 0.8935 - val\_mean\_absolute\_percentage\_error: 21.3603
Epoch 11/50
3156/3156 [==============================] - 1627s 515ms/step - loss: 0.0983 -
mean\_squared\_error: 0.0983 - mean\_absolute\_error: 0.2358 -
mean\_absolute\_percentage\_error: 5.7299 - val\_loss: 1.3811 - val\_mean\_squared\_error:
1.3811 - val\_mean\_absolute\_error: 0.9148 - val\_mean\_absolute\_percentage\_error: 21.6038
Epoch 12/50
3156/3156 [==============================] - 1425s 451ms/step - loss: 0.0882 -
mean\_squared\_error: 0.0882 - mean\_absolute\_error: 0.2288 -
mean\_absolute\_percentage\_error: 5.5620 - val\_loss: 1.6550 - val\_mean\_squared\_error:
1.6550 - val\_mean\_absolute\_error: 1.0449 - val\_mean\_absolute\_percentage\_error: 27.2221
Epoch 13/50
3156/3156 [==============================] - 1433s 454ms/step - loss: 0.1038 -
mean\_squared\_error: 0.1038 - mean\_absolute\_error: 0.2517 -
mean\_absolute\_percentage\_error: 6.1282 - val\_loss: 1.3538 - val\_mean\_squared\_error:
1.3538 - val\_mean\_absolute\_error: 0.9073 - val\_mean\_absolute\_percentage\_error: 21.6787
Epoch 14/50
3156/3156 [==============================] - 1485s 470ms/step - loss: 0.0963 -
mean\_squared\_error: 0.0963 - mean\_absolute\_error: 0.2375 -
mean\_absolute\_percentage\_error: 5.7705 - val\_loss: 1.3314 - val\_mean\_squared\_error:
1.3314 - val\_mean\_absolute\_error: 0.9068 - val\_mean\_absolute\_percentage\_error: 21.9359
Epoch 15/50
3156/3156 [==============================] - 1379s 437ms/step - loss: 0.0776 -
mean\_squared\_error: 0.0776 - mean\_absolute\_error: 0.2152 -
mean\_absolute\_percentage\_error: 5.1893 - val\_loss: 1.3625 - val\_mean\_squared\_error:
1.3625 - val\_mean\_absolute\_error: 0.9218 - val\_mean\_absolute\_percentage\_error: 22.7891
Epoch 16/50
3156/3156 [==============================] - 1355s 429ms/step - loss: 0.0677 -
mean\_squared\_error: 0.0677 - mean\_absolute\_error: 0.2009 -
mean\_absolute\_percentage\_error: 4.8694 - val\_loss: 1.3488 - val\_mean\_squared\_error:
1.3488 - val\_mean\_absolute\_error: 0.9197 - val\_mean\_absolute\_percentage\_error: 22.9609
Epoch 17/50
3156/3156 [==============================] - 1347s 427ms/step - loss: 0.0492 -
mean\_squared\_error: 0.0492 - mean\_absolute\_error: 0.1661 -
mean\_absolute\_percentage\_error: 4.0210 - val\_loss: 1.3188 - val\_mean\_squared\_error:
1.3188 - val\_mean\_absolute\_error: 0.8971 - val\_mean\_absolute\_percentage\_error: 21.2710
Epoch 18/50
3156/3156 [==============================] - 1352s 428ms/step - loss: 0.0589 -
mean\_squared\_error: 0.0589 - mean\_absolute\_error: 0.1847 -
mean\_absolute\_percentage\_error: 4.4815 - val\_loss: 1.5370 - val\_mean\_squared\_error:
1.5370 - val\_mean\_absolute\_error: 0.9943 - val\_mean\_absolute\_percentage\_error: 25.6795
Epoch 19/50
3156/3156 [==============================] - 1351s 428ms/step - loss: 0.0751 -
mean\_squared\_error: 0.0751 - mean\_absolute\_error: 0.1994 -
mean\_absolute\_percentage\_error: 4.7945 - val\_loss: 1.3511 - val\_mean\_squared\_error:
1.3511 - val\_mean\_absolute\_error: 0.9101 - val\_mean\_absolute\_percentage\_error: 22.0959
Epoch 20/50
3156/3156 [==============================] - 1347s 427ms/step - loss: 0.0584 -
mean\_squared\_error: 0.0584 - mean\_absolute\_error: 0.1849 -
mean\_absolute\_percentage\_error: 4.4394 - val\_loss: 1.3051 - val\_mean\_squared\_error:
1.3051 - val\_mean\_absolute\_error: 0.8975 - val\_mean\_absolute\_percentage\_error: 22.1280
Epoch 21/50
3156/3156 [==============================] - 1355s 429ms/step - loss: 0.0450 -
mean\_squared\_error: 0.0450 - mean\_absolute\_error: 0.1627 -
mean\_absolute\_percentage\_error: 3.9105 - val\_loss: 1.3927 - val\_mean\_squared\_error:
1.3927 - val\_mean\_absolute\_error: 0.9323 - val\_mean\_absolute\_percentage\_error: 23.6012
Epoch 22/50
3156/3156 [==============================] - 1350s 428ms/step - loss: 0.0464 -
mean\_squared\_error: 0.0464 - mean\_absolute\_error: 0.1605 -
mean\_absolute\_percentage\_error: 3.8682 - val\_loss: 1.4206 - val\_mean\_squared\_error:
1.4206 - val\_mean\_absolute\_error: 0.9458 - val\_mean\_absolute\_percentage\_error: 24.1697
Epoch 23/50
3156/3156 [==============================] - 1350s 428ms/step - loss: 0.0538 -
mean\_squared\_error: 0.0538 - mean\_absolute\_error: 0.1781 -
mean\_absolute\_percentage\_error: 4.3033 - val\_loss: 1.4779 - val\_mean\_squared\_error:
1.4779 - val\_mean\_absolute\_error: 0.9645 - val\_mean\_absolute\_percentage\_error: 24.8907
Epoch 24/50
3156/3156 [==============================] - 1352s 428ms/step - loss: 0.0875 -
mean\_squared\_error: 0.0875 - mean\_absolute\_error: 0.2275 -
mean\_absolute\_percentage\_error: 5.4662 - val\_loss: 1.3924 - val\_mean\_squared\_error:
1.3924 - val\_mean\_absolute\_error: 0.9276 - val\_mean\_absolute\_percentage\_error: 23.3362
Epoch 25/50
3156/3156 [==============================] - 1360s 431ms/step - loss: 0.0815 -
mean\_squared\_error: 0.0815 - mean\_absolute\_error: 0.2182 -
mean\_absolute\_percentage\_error: 5.2428 - val\_loss: 1.3398 - val\_mean\_squared\_error:
1.3398 - val\_mean\_absolute\_error: 0.9015 - val\_mean\_absolute\_percentage\_error: 22.0077
Epoch 26/50
3156/3156 [==============================] - 1348s 427ms/step - loss: 0.0580 -
mean\_squared\_error: 0.0580 - mean\_absolute\_error: 0.1824 -
mean\_absolute\_percentage\_error: 4.3500 - val\_loss: 1.3103 - val\_mean\_squared\_error:
1.3103 - val\_mean\_absolute\_error: 0.9012 - val\_mean\_absolute\_percentage\_error: 22.5496
Epoch 27/50
3156/3156 [==============================] - 1348s 427ms/step - loss: 0.0407 -
mean\_squared\_error: 0.0407 - mean\_absolute\_error: 0.1524 -
mean\_absolute\_percentage\_error: 3.6618 - val\_loss: 1.3086 - val\_mean\_squared\_error:
1.3086 - val\_mean\_absolute\_error: 0.8980 - val\_mean\_absolute\_percentage\_error: 22.1823
Epoch 28/50
3156/3156 [==============================] - 1347s 427ms/step - loss: 0.0321 -
mean\_squared\_error: 0.0321 - mean\_absolute\_error: 0.1374 -
mean\_absolute\_percentage\_error: 3.2948 - val\_loss: 1.3226 - val\_mean\_squared\_error:
1.3226 - val\_mean\_absolute\_error: 0.8963 - val\_mean\_absolute\_percentage\_error: 21.6126
Epoch 29/50
3156/3156 [==============================] - 1350s 428ms/step - loss: 0.0323 -
mean\_squared\_error: 0.0323 - mean\_absolute\_error: 0.1366 -
mean\_absolute\_percentage\_error: 3.2644 - val\_loss: 1.2806 - val\_mean\_squared\_error:
1.2806 - val\_mean\_absolute\_error: 0.8844 - val\_mean\_absolute\_percentage\_error: 21.5928
Epoch 30/50
3156/3156 [==============================] - 1356s 430ms/step - loss: 0.0377 -
mean\_squared\_error: 0.0377 - mean\_absolute\_error: 0.1458 -
mean\_absolute\_percentage\_error: 3.4816 - val\_loss: 1.2850 - val\_mean\_squared\_error:
1.2850 - val\_mean\_absolute\_error: 0.8874 - val\_mean\_absolute\_percentage\_error: 21.5136
Epoch 31/50
3156/3156 [==============================] - 1353s 429ms/step - loss: 0.0432 -
mean\_squared\_error: 0.0432 - mean\_absolute\_error: 0.1565 -
mean\_absolute\_percentage\_error: 3.7519 - val\_loss: 1.3319 - val\_mean\_squared\_error:
1.3319 - val\_mean\_absolute\_error: 0.8984 - val\_mean\_absolute\_percentage\_error: 21.6392
Epoch 32/50
3156/3156 [==============================] - 1342s 425ms/step - loss: 0.0422 -
mean\_squared\_error: 0.0422 - mean\_absolute\_error: 0.1567 -
mean\_absolute\_percentage\_error: 3.7381 - val\_loss: 1.3490 - val\_mean\_squared\_error:
1.3490 - val\_mean\_absolute\_error: 0.9169 - val\_mean\_absolute\_percentage\_error: 23.2494
Epoch 33/50
3156/3156 [==============================] - 1342s 425ms/step - loss: 0.0367 -
mean\_squared\_error: 0.0367 - mean\_absolute\_error: 0.1468 -
mean\_absolute\_percentage\_error: 3.5103 - val\_loss: 1.3181 - val\_mean\_squared\_error:
1.3181 - val\_mean\_absolute\_error: 0.8954 - val\_mean\_absolute\_percentage\_error: 21.8857
Epoch 34/50
3156/3156 [==============================] - 1346s 427ms/step - loss: 0.0312 -
mean\_squared\_error: 0.0312 - mean\_absolute\_error: 0.1337 -
mean\_absolute\_percentage\_error: 3.2084 - val\_loss: 1.3103 - val\_mean\_squared\_error:
1.3103 - val\_mean\_absolute\_error: 0.8976 - val\_mean\_absolute\_percentage\_error: 22.3398
Epoch 35/50
3156/3156 [==============================] - 1343s 425ms/step - loss: 0.0308 -
mean\_squared\_error: 0.0308 - mean\_absolute\_error: 0.1341 -
mean\_absolute\_percentage\_error: 3.2157 - val\_loss: 1.3503 - val\_mean\_squared\_error:
1.3503 - val\_mean\_absolute\_error: 0.9126 - val\_mean\_absolute\_percentage\_error: 22.7862
Epoch 36/50
3156/3156 [==============================] - 1347s 427ms/step - loss: 0.0409 -
mean\_squared\_error: 0.0409 - mean\_absolute\_error: 0.1541 -
mean\_absolute\_percentage\_error: 3.6624 - val\_loss: 1.2973 - val\_mean\_squared\_error:
1.2973 - val\_mean\_absolute\_error: 0.8907 - val\_mean\_absolute\_percentage\_error: 21.8468
Epoch 37/50
3156/3156 [==============================] - 1354s 429ms/step - loss: 0.0428 -
mean\_squared\_error: 0.0428 - mean\_absolute\_error: 0.1596 -
mean\_absolute\_percentage\_error: 3.8293 - val\_loss: 1.2957 - val\_mean\_squared\_error:
1.2957 - val\_mean\_absolute\_error: 0.8892 - val\_mean\_absolute\_percentage\_error: 21.7912
Epoch 38/50
3156/3156 [==============================] - 1395s 442ms/step - loss: 0.0477 -
mean\_squared\_error: 0.0477 - mean\_absolute\_error: 0.1672 -
mean\_absolute\_percentage\_error: 4.0116 - val\_loss: 1.3319 - val\_mean\_squared\_error:
1.3319 - val\_mean\_absolute\_error: 0.8999 - val\_mean\_absolute\_percentage\_error: 21.6848
Epoch 39/50
3156/3156 [==============================] - 1356s 430ms/step - loss: 0.0587 -
mean\_squared\_error: 0.0587 - mean\_absolute\_error: 0.1869 -
mean\_absolute\_percentage\_error: 4.5054 - val\_loss: 1.3270 - val\_mean\_squared\_error:
1.3270 - val\_mean\_absolute\_error: 0.8961 - val\_mean\_absolute\_percentage\_error: 21.3831
Epoch 40/50
3156/3156 [==============================] - 1421s 450ms/step - loss: 0.0450 -
mean\_squared\_error: 0.0450 - mean\_absolute\_error: 0.1597 -
mean\_absolute\_percentage\_error: 3.8111 - val\_loss: 1.2846 - val\_mean\_squared\_error:
1.2846 - val\_mean\_absolute\_error: 0.8894 - val\_mean\_absolute\_percentage\_error: 21.5802
Epoch 41/50
3156/3156 [==============================] - 1438s 456ms/step - loss: 0.0304 -
mean\_squared\_error: 0.0304 - mean\_absolute\_error: 0.1295 -
mean\_absolute\_percentage\_error: 3.0991 - val\_loss: 1.3462 - val\_mean\_squared\_error:
1.3462 - val\_mean\_absolute\_error: 0.9131 - val\_mean\_absolute\_percentage\_error: 22.8258
Epoch 42/50
3156/3156 [==============================] - 1379s 437ms/step - loss: 0.0288 -
mean\_squared\_error: 0.0288 - mean\_absolute\_error: 0.1285 -
mean\_absolute\_percentage\_error: 3.0693 - val\_loss: 1.3356 - val\_mean\_squared\_error:
1.3356 - val\_mean\_absolute\_error: 0.9080 - val\_mean\_absolute\_percentage\_error: 22.6378
Epoch 43/50
3156/3156 [==============================] - 1549s 491ms/step - loss: 0.0327 -
mean\_squared\_error: 0.0327 - mean\_absolute\_error: 0.1381 -
mean\_absolute\_percentage\_error: 3.2934 - val\_loss: 1.2731 - val\_mean\_squared\_error:
1.2731 - val\_mean\_absolute\_error: 0.8818 - val\_mean\_absolute\_percentage\_error: 21.1893
Epoch 44/50
3156/3156 [==============================] - 1485s 471ms/step - loss: 0.0305 -
mean\_squared\_error: 0.0305 - mean\_absolute\_error: 0.1338 -
mean\_absolute\_percentage\_error: 3.1919 - val\_loss: 1.2890 - val\_mean\_squared\_error:
1.2890 - val\_mean\_absolute\_error: 0.8852 - val\_mean\_absolute\_percentage\_error: 21.1778
Epoch 45/50
3156/3156 [==============================] - 1451s 460ms/step - loss: 0.0291 -
mean\_squared\_error: 0.0291 - mean\_absolute\_error: 0.1301 -
mean\_absolute\_percentage\_error: 3.1152 - val\_loss: 1.3055 - val\_mean\_squared\_error:
1.3055 - val\_mean\_absolute\_error: 0.8914 - val\_mean\_absolute\_percentage\_error: 21.8665
Epoch 46/50
3156/3156 [==============================] - 1607s 509ms/step - loss: 0.0313 -
mean\_squared\_error: 0.0313 - mean\_absolute\_error: 0.1358 -
mean\_absolute\_percentage\_error: 3.2478 - val\_loss: 1.2845 - val\_mean\_squared\_error:
1.2845 - val\_mean\_absolute\_error: 0.8828 - val\_mean\_absolute\_percentage\_error: 21.1241
Epoch 47/50
3156/3156 [==============================] - 2373s 752ms/step - loss: 0.0389 -
mean\_squared\_error: 0.0389 - mean\_absolute\_error: 0.1493 -
mean\_absolute\_percentage\_error: 3.5678 - val\_loss: 1.3075 - val\_mean\_squared\_error:
1.3075 - val\_mean\_absolute\_error: 0.9029 - val\_mean\_absolute\_percentage\_error: 22.5372
Epoch 48/50
3156/3156 [==============================] - 1583s 502ms/step - loss: 0.0360 -
mean\_squared\_error: 0.0360 - mean\_absolute\_error: 0.1415 -
mean\_absolute\_percentage\_error: 3.3586 - val\_loss: 1.3113 - val\_mean\_squared\_error:
1.3113 - val\_mean\_absolute\_error: 0.8990 - val\_mean\_absolute\_percentage\_error: 22.1704
Epoch 49/50
3156/3156 [==============================] - 1389s 440ms/step - loss: 0.0554 -
mean\_squared\_error: 0.0554 - mean\_absolute\_error: 0.1824 -
mean\_absolute\_percentage\_error: 4.3745 - val\_loss: 1.3967 - val\_mean\_squared\_error:
1.3967 - val\_mean\_absolute\_error: 0.9286 - val\_mean\_absolute\_percentage\_error: 23.2085
Epoch 50/50
3156/3156 [==============================] - 1405s 445ms/step - loss: 0.0494 -
mean\_squared\_error: 0.0494 - mean\_absolute\_error: 0.1706 -
mean\_absolute\_percentage\_error: 4.0780 - val\_loss: 1.3150 - val\_mean\_squared\_error:
1.3150 - val\_mean\_absolute\_error: 0.8972 - val\_mean\_absolute\_percentage\_error: 22.1429

    \end{Verbatim}
\newpage
\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} <keras.callbacks.History at 0x22bed63940>
\end{Verbatim}
            
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}237}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,}\PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train\PYZus{}mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{vgg16\PYZus{}adapted}\PY{o}{.}\PY{n}{history}\PY{o}{.}\PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}mse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comparison of Training MSE and Validation MSE in Adapted VGG16 Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epochs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_69_0.png}
    \end{center}
    
    \subsubsection{Analysis of Training Loss vs Validation Loss}

The training loss decreases as the epochs increase, but the validation
loss flucatues around 1.0 - 1.5. This indicates that the model has
overfit to the data, which as previously explained, was expected due to
the small sample size and variation between each data point (artwork).\\

While overfit could also be due to the number of epochs being too high,
in which case an early-stopping mechanism could be implemented, this is
probably not likely given that the validation error is not dipping and
rising again after a certain point, but does not even start to go down
at any point. As such, while the CNN is overfitting, an early-stopping
criterion will not assist in this case.

    \subsubsection{Making Predictions on Training and Testing Data}

Also the results above show that the model has overfitted to the
training data, I decided to evaluate the model's predictions on the
training data and unseen testing data anyway to gain more insight as to
how poorly it is performing on the testing data.
\newpage
   \subsubsection*{Training Data Predictions}

The 5 examples below show that the training predictions are performing
well, even guessing the exact number at some points. Unfortunately, this
is a bad sign since it just cements the fact that the model has
overfitted.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}123}]:} \PY{n}{cnn\PYZus{}train\PYZus{}pred}\PY{p}{,} \PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{,} \PY{n}{cnn\PYZus{}train\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{vgg16\PYZus{}adapted}\PY{p}{,}
          \PY{n}{cnn\PYZus{}X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}124}]:} \PY{n}{cnn\PYZus{}train\PYZus{}pred\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{cnn\PYZus{}train\PYZus{}pred}\PY{p}{)}
          \PY{n}{y\PYZus{}train\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Real Faves: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{np.floor(y\PYZus{}train\PYZus{}exp[i])\PYZcb{}, }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Predicted Faves:}
          \PY{p}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n}{cnn\PYZus{}train\PYZus{}pred\PYZus{}exp}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cnn\PYZus{}X\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_73_0.png}
    \end{center}
    
       \subsubsection*{Testing Data Predictions}

While the 5 testing data predictions below are just examples and not
representative of the model's performance on the entire dataset, we can
still see that the model's predictions are not performing as well as on
the training data.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}125}]:} \PY{n}{cnn\PYZus{}test\PYZus{}pred}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}line} \PY{o}{=} \PY{n}{predict\PYZus{}score}\PY{p}{(}\PY{n}{vgg16\PYZus{}adapted}\PY{p}{,} \PY{n}{cnn\PYZus{}X\PYZus{}test}\PY{p}{,}
          \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}126}]:} \PY{n}{cnn\PYZus{}test\PYZus{}pred\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{cnn\PYZus{}test\PYZus{}pred}\PY{p}{)}
          \PY{n}{y\PYZus{}test\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{False}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Real Faves: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{np.floor(y\PYZus{}test\PYZus{}exp[i])\PYZcb{}, }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Predicted Faves:}
          \PY{p}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{floor}\PY{p}{(}\PY{n}{cnn\PYZus{}test\PYZus{}pred\PYZus{}exp}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cnn\PYZus{}X\PYZus{}test}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_76_0.png}
    \end{center}
    
    \newpage
       \subsubsection{Analysis of Predicted vs Observed Values}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,}
          \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted vs Observed Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.04}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Observed Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ha}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
          \PY{n}{fig}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{l+m+mf}{0.04}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Values}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{va}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vertical}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training set predictions (}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{len(y\PYZus{}train)\PYZcb{} data points)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{cnn\PYZus{}train\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data points}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cnn\PYZus{}train\PYZus{}pred}\PY{p}{,} \PY{n}{cnn\PYZus{}train\PYZus{}line}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Testing set predictions (}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{len(y\PYZus{}test)\PYZcb{} data points)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}pred}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data points}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{cnn\PYZus{}test\PYZus{}pred}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}line}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coral}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} <matplotlib.legend.Legend at 0x1a3473f898>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{DA-analysis_files/DA-analysis_78_1.png}
    \end{center}
    
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}239}]:} \PY{n}{cnn\PYZus{}results\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Testing set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2 score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Explained variance score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean Squared Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean Absolute Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Median Absolute Error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{n}{cnn\PYZus{}train\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{n}{cnn\PYZus{}test\PYZus{}scores}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{]}\PY{p}{,}
          \PY{p}{\PYZcb{}}\PY{p}{)}
          
          \PY{n}{cnn\PYZus{}results\PYZus{}df}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.0\linewidth}{1.2\paperheight}}{DA-analysis_files/cnn-scores.png}
    \end{center}
            \newpage
    \subsubsection{Summary}

The MSE comparison plot shows that the model has most likely overfitted
the data since the training MSE has successfully decreased whereas the
validation MSE remains consistently around 1.0 - 1.5 with no sign of
decrease. This is also reflected in the predicted vs observed values
regression plots. The model's training data predictions have a good
regression line fit to the observed values, whereas the model's testing
data predictions have pretty much no pattern to them. The results
dataframe also shows this in the explained variance score and errors.
While the model has 0.7 explained variance on the training data, it has
-0.28 explained variance for the testing data. Furthermore, while the
MAE is relatively low on the training data (0.29), the MAE on the
testing data is 0.91.\\

The overfitting is probably a result of the small sample size. Aside
from simply collecting more data, some other suggestions for improvement
are listed below.\footnote{\textbf{\#creativeheuristics:} While an obvious solution to overfitting would be to collect more data, there are various other steps that can be implemented to try to minimize the model's overfitting behavior. By doing research and exploring various different concepts (augmenting the data, reducing dimensionality, using neural codes, including dropout layer, etc.), I demonstrate an integration of multiple different heuristics in order to solve the problem of overfitting. By exploring various methods for combatting overfit, I gained a better understanding of why overfit happens, why it could be happening to my data, and also am now aware of various steps I can try to reduce overfit in my model.}

\subsubsection{Potential Improvements for CNN Model}

\subsubsection*{Selective Filtering of Images}

While most of the images look fine when resized to 250 x 250, there are
some images with very unbalanced height x width dimensions that look
severely misrepresented after resizing. While this is probably not a
significant contributer to the model's poor performance on unseen data,
it would still be good to remove such images beforehand.

\subsubsection*{Using PCA to Reduce Dimensionality vs Naive Resizing}

To make my images have similar input sizes, I simply used
\texttt{skimage.transform} to resize my images to 250 x 250. By using a
more sophisticated method for dimensionality reduction that actually
seeks to maximize the explained variance, such as PCA, I could
potentially have retained more information in my images after reducing
their dimensionality.

\subsubsection*{Data Augmentation}

Data augmentation is a technique for increasing the dataset size and
also improving the robustness of the CNN model by making it invariant to
different forms of translations and transformations (Raj, 2018). Some
data augmentation methods include flipping, rotating, scaling, cropping,
etc. Since I am dealing with art popularity and not object
classification, the augmentation will have to carried out within
reasonable constraints since the same piece of art, cropped, will most
definitely not represent the original art accurately. Minor augmentation
such as flipping and rotation should be fine and may help the model
since we do not expect an artwork's aesthetic appeal to significantly
decrease if it was viewed from a different orientation.

\subsubsection*{Include Dropout Layer}

Dropout is a regularization method that randomly ignores various units
in the neural network during the training process; it has been shown to
correct overfitting in convolutional neural networks (Srivastava,
Hinton, Krizhevsky, Sutskever, \& Salakhutdinov, 2014). By randomly
dropping nodes during training, the other nodes within the layer are
forced to probabilistically take on more or less responsibility for the
inputs. This minimizes the ocurrence of complex co-adaptations within
the network, which can be hard to generalize to unseen data (Srivastava
et al., 2014).\\

While the original VGG16 model has dropout regularization incorporated
(Simonyan \& Zisserman, 2014), the Keras VGG16 does not. As such, I
could try incorporating dropout layers in the future to reduce
overfitting.

\subsubsection*{Create Image Representation using Lower-Layer Neural Codes}

Neural Codes are "the activations invoked by an image within the top
layers of a large convolutional neural network {[}that{]} provide a
high-level descriptor of the visual content of the image" (Babenko,
Slesarev, Chigorin, \& Lempitsky, 2014). In a study on the efficacy and
applicability of neural codes for image classification in different
tasks, Babenko et al. (2014) found that neural codes trained on a CNN
for a particular task could transfer well to classification performance
on a different task.\\

Of particular note, they found that "the best performance is observed
not on the very top of the network, but rather at the layer that is two
levels below the outputs ... because the very top layers are too much
tuned for the classification task, while the bottom layers do not
acquire enough invariance to nuisance factors" (Babenko et al., 2014).
Their findings suggest that when implementing transfer-learning, I
should consider using lower-layer neural codes instead and train a
separate model on those neural codes to avoid the VGG16 model from
overfitting to various noise factors within my images (since VGG16 was
originally built for a different task).\\

Babenko et al. (2014) also found that PCA could be used to reduce the
dimensionality of their neural codes to as low as 250 or even 180
dimensions without loss in model performance. Thus, in the future, I
could consider using neural codes combined with PCA dimensionality
reduction to create image representations that hopefully retain the most
variance while discarding the most noise to build a model that does not
overfit.

\subsubsection*{Early-stopping (Not Applicable in this Case)}

Early-stopping is a method for preventing overfit by stopping the
training as soon as the validation error is higher than the last time
that it was checked (Prechelt, 1998). The weights of the model at that
point in time are then used as the final model weights for testing.
While this is a useful method to make note of in general, it does not
apply in this specific case since the validation error does not show a
pattern of decrease followed by increase.
\newpage
    \section{Conclusion}

While my results show that the artist attributes model has a better
performance than the image features model, I would say that comparing
the different models is not quite fair since the adapted VGG16 model has
much more parameters vs the amount of data available, which leads to a
much higher chance of overfitting - as we see in the results. In the
future, I would need to collect more data as well as incorporate various
other strategies to combat overfitting, e.g. data augmentation, dropout.\\

That being said, the preliminary results obtained from the artist
attributes models are quite encouraging as they align with our
intuitions that the more followers and page views an artist has, the
more favorites they will receive on their image. Although the mean
absolute error is \textasciitilde{}70\%, I think given the use case, a
highly accurate model is not necessary, and we would instead like to
gain an intuition as to the most influential predictors for fanart
popularity.\\

By knowing the artist attributes with the most predictive power, we see
that the underlying behavior within fans of fictional works is not too
different from that of social media, i.e. more followers == more likes.
In future iterations of this work, gathering more data may reveal other
social metrics that are good predictors, and allow those in the media
and entertainment industry to know which social metrics they should
prioritize when looking to collaborate with a particular artist for
advertising, to increase the exposure and popularity towards a certain
show, movie, etc.\footnote{\textbf{\#regression:} This whole project consisted of solving a regression task. In order to predict the popularity of fanart, the number of favorites is used as a response variable, and various models - from linear regression models to adapted CNNs - are used to solve the regression problem by testing out different types of predictors (artist attributes vs image features).}

\subsection{Potential Improvements and Extensions}

While specific improvements for the CNN model were listed above, the
improvements and extensions listed here are more general and apply to
the project as a whole.

\subsubsection{Potential Improvements}

\begin{itemize}
\item
  Normalize the number of favorites to account for the effect of account
  age. According to Khosla et al. (2014), "visual media tends to receive
  views over some period of time. To normalize for this effect, we
  divide the number of views by the duration since the upload date of
  the given image."
\item
  Use a more nuanced "popularity indicator" instead of just using the
  number of favorites. While using the number of favorites is easy and
  convenient for analyzing popularity, it is possible that the number of
  views may be more suggestive of popularity since to favorite an image,
  one has to be a member of the site, whereas anyone can view the image.
  As such, a more nuanced popularity metric consisting of some mixture
  of the two may be ideal; perhaps even comments could be included since
  they indicate engagement with the image.
\end{itemize}
\newpage
\subsubsection{Potential Extensions}

\begin{itemize}
\item
  Analyze high-level color features by extracting the images color
  distribution, hues, intensity, etc. This would provide a more
  interpretable indication of 'popular' image features vs using a CNN.
\item
  Analyze the different types of comments on the image. It would be
  interesting to use NLP techniques to analyze the different comment
  types and try to categorize them into good, bad, or neutral comments -
  which can then be used to predict image popularity.
\item
  Analyze the image titles. While the image titles were not used as a
  predictor this time (mainly because I did not think that the length of
  the image title would be a very good predictor), they could be used in
  the future.
\item
  Analyze the hashtags. As previously explained, I could not use the
  hashtags in the predictive process this time since there were so few
  non-null values, and I also made a mistake while scraping the
  hashtags, thus corrupting the data. That being said, I think hashtags
  would be a potential powerful predictor - one could use the number of
  hashtags as a predictor, and also the type of hashtags if they are
  processed using NLP methods.
\item
  Analyze image description. Every image posted on Deviantart comes with
  a description written by the artist. I did not scrape or analyze image
  descriptions in this project since the descriptions often contained
  irregular elements such as images, emojis, gifs, etc. which made them
  difficult to parse into a proper format. That being said, descriptions
  - much like hashtags - would also be a significant feature to consider
  in the future in terms of both length and content.
\item
  Analyze age, sex, location. While I did scrape this data, the column
  format was deemed to challenging to parse this time around. In the
  future, I would work to parse the column properly to gain insights
  from the artist's more personal attributes.
\item
  Analyze artist activity. Artist activity, such as how often they post,
  comment, etc. could also be a strong predictor. This feature would be
  quite hard to obtain since most artists tend to keep their activity
  hidden on DeviantArt.
\end{itemize}
\newpage
\begin{thebibliography}{9}

\bibitem{neuralcodes}
Babenko, A., Slesarev, A., Chigorin, A., \& Lempitsky, V. (2014,
September). Neural codes for image retrieval. In European conference on
computer vision (pp. 584-599). Springer, Cham.

\bibitem{instagram}
De Veirman, M., Cauberghe, V., \& Hudders, L. (2017). Marketing through
Instagram influencers: the impact of number of followers and product
divergence on brand attitude. International Journal of Advertising,
36(5), 798-828.

\bibitem{deviantart}
DeviantArt. (2019). Copyright Policy. Retrieved March 16, 2019, from
https://about.deviantart.com/policy/copyright/

\bibitem{dufour}
Dufour, N. (2014). Will it play in Peoria? Predicting Image Popularity with Convolutional Neural Networks. Retrieved March 25, 2019, from http://cs231n.stanford.edu/reports/2015/pdfs/dufour.pdf

\bibitem{googletrends}
Google Trends (2019) Sherlock Holmes Worldwide Interest Over time.
Retrieved April 23, 2019, from
https://trends.google.com/trends/explore?date=all\&q=\%2Fm\%2F06rkl

\bibitem{gwr}
Guinness World Record. (2012, May 14). Sherlock Holmes awarded title for
most portrayed literary human character in film \& TV. Retrieved April
21, 2019, from
http://www.guinnessworldrecords.com/news/2012/5/sherlock-holmes-awarded-title-for-most-portrayed-literary-human-character-in-film-tv-41743/

\bibitem{vgg16}
Hassan, M. U. (2018, November 21). VGG16 - Convolutional Network for
Classification and Detection. Retrieved April 21, 2019, from
https://neurohive.io/en/popular-networks/vgg16/

\bibitem{cnnimages}
Karpathy, A. (2016). Cs231n convolutional neural networks for visual
recognition. Neural networks, 1. Retrieved April 25, 2019, from
https://cs231n.github.io/convolutional-networks/

\bibitem{khosla}
Khosla, A., Das Sarma, A., \& Hamid, R. (2014, April). What makes an
image popular?. In Proceedings of the 23rd international conference on
World wide web (pp. 867-876). ACM.

\bibitem{imagenet}
Krizhevsky, A., Sutskever, I., \& Hinton, G. E. (2012). Imagenet
classification with deep convolutional neural networks. In Advances in
neural information processing systems (pp. 1097-1105).

\bibitem{earlystopping}
Prechelt, L. (1998). Early stopping-but when?. In Neural Networks:
Tricks of the trade (pp. 55-69). Springer, Berlin, Heidelberg.

\bibitem{dataaug}
Raj, B. (2018, April 11). Data Augmentation \textbar{} How to use Deep
Learning when you have Limited Data - Part 2. Retrieved April 23, 2019,
from
https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced

\bibitem{cnn}
Simonyan, K., \& Zisserman, A. (2014). Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556.

\bibitem{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \&
Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural
networks from overfitting. The Journal of Machine Learning Research,
15(1), 1929-1958.

\bibitem{fanart}
Wikipedia contributors. (2019, April 13). Fan art. In Wikipedia, The
Free Encyclopedia. Retrieved 03:11, April 21, 2019, from
https://en.wikipedia.org/w/index.php?title=Fan\_art\&oldid=892258552

\bibitem{sherlock}
Wikipedia contributors. (2019, April 20). Sherlock Holmes. In Wikipedia,
The Free Encyclopedia. Retrieved 16:10, April 21, 2019, from
https://en.wikipedia.org/w/index.php?title=Sherlock\_Holmes\&oldid=893342542

\end{thebibliography}
\newpage
\appendix
\section{Appendix: Scraper Code}
\label{appendix:scraper}
\url{https://nbviewer.jupyter.org/github/hueyning/DA-scraper/blob/master/python-notebook/DA-webscraper.ipynb}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{}import basic libraries}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{k+kn}{import} \PY{n+nn}{logging}
        \PY{k+kn}{import} \PY{n+nn}{scrapy}
        \PY{k+kn}{from} \PY{n+nn}{scrapy}\PY{n+nn}{.}\PY{n+nn}{crawler} \PY{k}{import} \PY{n}{CrawlerProcess}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{scrapy}\PY{n+nn}{.}\PY{n+nn}{pipelines}\PY{n+nn}{.}\PY{n+nn}{images} \PY{k}{import} \PY{n}{ImagesPipeline}
        \PY{k+kn}{from} \PY{n+nn}{scrapy}\PY{n+nn}{.}\PY{n+nn}{exceptions} \PY{k}{import} \PY{n}{DropItem}
        
        \PY{k}{class} \PY{n+nc}{ImageItem}\PY{p}{(}\PY{n}{scrapy}\PY{o}{.}\PY{n}{Item}\PY{p}{)}\PY{p}{:}
        
            \PY{c+c1}{\PYZsh{}direct link to image file for downloading via ImagePipeline}
            \PY{n}{image\PYZus{}urls} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{}link to specific image page for scraping more stats}
            \PY{n}{image\PYZus{}links} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{}image attributes}
            \PY{n}{titles} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}image title}
            \PY{n}{date\PYZus{}posted} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}date posted}
            \PY{n}{hashtags} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}hashtags}
        
            \PY{c+c1}{\PYZsh{}image stats}
            \PY{n}{views} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of views of the current image}
            \PY{n}{faves} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of faves of the current image}
            \PY{n}{comments} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of comments of the current image}
            \PY{n}{downloads} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of downloads of the current image}
        
            \PY{c+c1}{\PYZsh{}artist details}
            \PY{n}{artists} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}artist\PYZsq{}s name}
            \PY{n}{artist\PYZus{}urls} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}link to the artist\PYZsq{}s account}
            \PY{n}{artist\PYZus{}deviations} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of deviations (images) posted}
            \PY{n}{artist\PYZus{}comments} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of total comments received}
            \PY{n}{artist\PYZus{}page\PYZus{}views} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of total page views received}
            \PY{n}{artist\PYZus{}scraps} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of scraps (WIPs or archived art)}
            \PY{n}{artist\PYZus{}watchers} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of watchers (followers)}
            \PY{n}{artist\PYZus{}critiques} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of critiques given}
            \PY{n}{artist\PYZus{}forum\PYZus{}posts} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of forum posts made}
            \PY{n}{artist\PYZus{}faves} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}number of total faves received}
            \PY{n}{artist\PYZus{}asl} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}age, sex, location}
            \PY{n}{artist\PYZus{}dob} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}date of birth}
            \PY{n}{account\PYZus{}age} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}how old the account is}
        
            \PY{c+c1}{\PYZsh{} to be filled in by ImagePipeline}
            \PY{n}{image\PYZus{}paths} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Field}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}location of image in local storage}
        
        
        \PY{k}{class} \PY{n+nc}{MyImagesPipeline}\PY{p}{(}\PY{n}{ImagesPipeline}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Image pipeline for downloading images.}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{k}{def} \PY{n+nf}{get\PYZus{}media\PYZus{}requests}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{item}\PY{p}{,} \PY{n}{info}\PY{p}{)}\PY{p}{:}
                \PY{k}{for} \PY{n}{image\PYZus{}url} \PY{o+ow}{in} \PY{n}{item}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
                    \PY{k}{yield} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Request}\PY{p}{(}\PY{n}{image\PYZus{}url}\PY{p}{)}
        
            \PY{k}{def} \PY{n+nf}{item\PYZus{}completed}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{results}\PY{p}{,} \PY{n}{item}\PY{p}{,} \PY{n}{info}\PY{p}{)}\PY{p}{:}
                \PY{n}{image\PYZus{}paths} \PY{o}{=} \PY{p}{[}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{path}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{ok}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n}{results} \PY{k}{if} \PY{n}{ok}\PY{p}{]}
                \PY{k}{if} \PY{o+ow}{not} \PY{n}{image\PYZus{}paths}\PY{p}{:}
                    \PY{k}{raise} \PY{n}{DropItem}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Item contains no images}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                \PY{n}{item}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZus{}paths}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{image\PYZus{}paths}
                \PY{k}{return} \PY{n}{item}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{class} \PY{n+nc}{ImageSpider}\PY{p}{(}\PY{n}{scrapy}\PY{o}{.}\PY{n}{Spider}\PY{p}{)}\PY{p}{:}
        
            \PY{n}{name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images}\PY{l+s+s1}{\PYZsq{}}
        
            \PY{n}{start\PYZus{}urls} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://www.deviantart.com/popular\PYZhy{}all\PYZhy{}time/?q=sherlock\PYZam{}offset=0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
            \PY{c+c1}{\PYZsh{}initialize offset at 0}
            \PY{n}{offset} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{c+c1}{\PYZsh{}set offset limit to control the amount of images downloaded}
            \PY{n}{offset\PYZus{}limit} \PY{o}{=} \PY{l+m+mi}{16000}
        
        
            \PY{n}{custom\PYZus{}settings} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LOG\PYZus{}LEVEL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{logging}\PY{o}{.}\PY{n}{INFO}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ITEM\PYZus{}PIPELINES}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}\PYZus{}main\PYZus{}\PYZus{}.MyImagesPipeline}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{c+c1}{\PYZsh{}enable image download}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IMAGES\PYZus{}STORE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DA\PYZhy{}images\PYZhy{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{}store images in DA\PYZhy{}images\PYZhy{}2 folder}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FEED\PYZus{}FORMAT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{FEED\PYZus{}URI}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image\PYZhy{}data\PYZhy{}2.json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{}store image data in image\PYZhy{}data\PYZhy{}2.json}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DOWNLOAD\PYZus{}FAIL\PYZus{}ON\PYZus{}DATALOSS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{False}\PY{p}{,} \PY{c+c1}{\PYZsh{}if image download fails (due to various}
        \PY{n}{issues}\PY{p}{)}\PY{p}{,} \PY{n}{don}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t send error message, just flag it.}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DOWNLOAD\PYZus{}DELAY}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mf}{0.25} \PY{c+c1}{\PYZsh{}250 ms download delay, with inbuilt scrapy randomization}
            \PY{p}{\PYZcb{}}
        
            \PY{k}{def} \PY{n+nf}{parse}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{response}\PY{p}{)}\PY{p}{:}
        
                \PY{c+c1}{\PYZsh{}get page body}
                \PY{n}{page} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{css}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{div.page\PYZhy{}results span.thumb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
                \PY{k}{for} \PY{n}{img} \PY{o+ow}{in} \PY{n}{page}\PY{p}{:}
        
                    \PY{c+c1}{\PYZsh{}thumbnail link}
                    \PY{n}{thumbnail} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{css}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{::attr(data\PYZhy{}super\PYZhy{}img)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
        
                    \PY{c+c1}{\PYZsh{}full link that leads to the individual image post}
                    \PY{n}{img\PYZus{}link} \PY{o}{=} \PY{n}{img}\PY{o}{.}\PY{n}{css}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{::attr(href)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{get}\PY{p}{(}\PY{p}{)}
        
                    \PY{c+c1}{\PYZsh{}if there is a thumbnail, aka the post is an image, follow url to scrape}
        \PY{n}{image} \PY{n}{details}
                    \PY{k}{if} \PY{n}{thumbnail}\PY{p}{:}
                        \PY{k}{yield} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Request}\PY{p}{(}\PY{n}{img\PYZus{}link}\PY{p}{,} \PY{n}{callback} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{parse\PYZus{}image}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{}go to next page}
                \PY{k}{while} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{offset} \PY{o}{\PYZlt{}} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{offset\PYZus{}limit}\PY{p}{:}
                    \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{offset} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{24} \PY{c+c1}{\PYZsh{}DA\PYZsq{}s natural offset scroll is set at increments of 24}
                    \PY{n}{next\PYZus{}page} \PY{o}{=} \PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://www.deviantart.com/popular\PYZhy{}all\PYZhy{}}
        \PY{n}{time}\PY{o}{/}\PY{err}{?}\PY{n}{q}\PY{o}{=}\PY{n}{sherlock}\PY{o}{\PYZam{}}\PY{n}{offset}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{offset}\PY{p}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
                    \PY{k}{yield} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Request}\PY{p}{(}\PY{n}{next\PYZus{}page}\PY{p}{,} \PY{n}{callback}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{parse}\PY{p}{)}
        
        
            \PY{k}{def} \PY{n+nf}{parse\PYZus{}image}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{response}\PY{p}{)}\PY{p}{:}
        
                \PY{c+c1}{\PYZsh{}initialize image item}
                \PY{n}{image} \PY{o}{=} \PY{n}{ImageItem}\PY{p}{(}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{}get image url (for downloading via ImagePipeline)}
                \PY{n}{image}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{image\PYZus{}urls}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{response}\PY{o}{.}\PY{n}{css}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{div.dev\PYZhy{}view\PYZhy{}deviation img}
        \PY{p}{:}\PY{p}{:}\PY{n}{attr}\PY{p}{(}\PY{n}{src}\PY{p}{)}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{).get()]}
                \PY{c+c1}{\PYZsh{}get other image info}
                \PY{n}{image}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{image\PYZus{}links}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{url}
                \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{titles}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//a[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{title}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{]/text()}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{extract}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
                \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date\PYZus{}posted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//div[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{dev\PYZhy{}right\PYZhy{}bar\PYZhy{}content dev\PYZhy{}}
        \PY{n}{metainfo}\PY{o}{\PYZhy{}}\PY{n}{content} \PY{n}{dev}\PY{o}{\PYZhy{}}\PY{n}{metainfo}\PY{o}{\PYZhy{}}\PY{n}{details}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/dl/dd/span/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()[0]}
        
                \PY{c+c1}{\PYZsh{}check whether image has hashtags (some don\PYZsq{}t)}
                \PY{n}{hashtag} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//div[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{dev\PYZhy{}about\PYZhy{}tags\PYZhy{}cc dev\PYZhy{}about\PYZhy{}}
        \PY{n}{breadcrumb}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/a/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()}
                \PY{k}{if} \PY{n}{hashtag}\PY{p}{:} \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{hashtag}
        
                \PY{c+c1}{\PYZsh{}get image stats}
                \PY{n}{stats} \PY{o}{=}  \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//div[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{dev\PYZhy{}right\PYZhy{}bar\PYZhy{}content dev\PYZhy{}metainfo\PYZhy{}}
        \PY{n}{content} \PY{n}{dev}\PY{o}{\PYZhy{}}\PY{n}{metainfo}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/dl/dd/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()}
        
                \PY{c+c1}{\PYZsh{}check that stats list only contains numbers (sometimes irregular data falls in)}
                \PY{n}{stats} \PY{o}{=} \PY{p}{[}\PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{s}\PY{p}{)} \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{stats}\PY{p}{]}
        
                \PY{c+c1}{\PYZsh{}remove any None types from list}
                \PY{n}{stats} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{stats}\PY{p}{)}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{}the responses are ordered in: views, faves, comments, downloads}
                \PY{c+c1}{\PYZsh{}sometimes comments are disabled, sometimes downloads are disabled}
                \PY{n}{headers} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{faves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{downloads}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
                \PY{c+c1}{\PYZsh{}if comments/downloads are disabled, they will not be looped over for a given}
        \PY{n}{image}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{stats}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{image}\PY{p}{[}\PY{n}{headers}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{stats}\PY{p}{[}\PY{n}{i}\PY{p}{]}
        
                \PY{c+c1}{\PYZsh{}get artist info}
                \PY{n}{artist\PYZus{}name} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//small[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{author}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{]/span/a/text()}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{extract}\PY{p}{(}\PY{p}{)}
        
                \PY{k}{if} \PY{n}{artist\PYZus{}name}\PY{p}{:}
                    \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artists}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=}
        \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//small[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{author}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{]/span/a/text()}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{extract}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=}
        \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//small[@class=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{author}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{]/span/a/@href}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{extract}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                    \PY{n}{request} \PY{o}{=} \PY{n}{scrapy}\PY{o}{.}\PY{n}{Request}\PY{p}{(}\PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}urls}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{callback}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{parse\PYZus{}artist}\PY{p}{,}
        \PY{n}{meta}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{image}\PY{p}{\PYZcb{}}\PY{p}{)}
                    \PY{k}{yield} \PY{n}{request}
                \PY{k}{else}\PY{p}{:} \PY{c+c1}{\PYZsh{}if no artist name (sometimes artists are banned), just yield the image}
                    \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artists}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Banned}\PY{l+s+s1}{\PYZsq{}}
                    \PY{k}{yield} \PY{n}{image}
        
            \PY{k}{def} \PY{n+nf}{parse\PYZus{}artist}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{response}\PY{p}{)}\PY{p}{:}
        
                \PY{c+c1}{\PYZsh{}get image item for the higher\PYZhy{}level parser}
                \PY{n}{image} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{meta}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
                \PY{c+c1}{\PYZsh{}get artist account stats}
                \PY{n}{artist\PYZus{}stats} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//div[@id=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{super\PYZhy{}secret\PYZhy{}}
        \PY{n}{stats}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/div/div/div/strong/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()}
        
                \PY{n}{headers} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}deviations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}comments}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}page\PYZus{}views}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}scr}
        \PY{n}{aps}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{n}{artist\PYZus{}watchers}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{n}{artist\PYZus{}critiques}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{n}{artist\PYZus{}forum\PYZus{}posts}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{n}{artist\PYZus{}faves}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]}
        
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{artist\PYZus{}stats}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{image}\PY{p}{[}\PY{n}{headers}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{artist\PYZus{}stats}\PY{p}{[}\PY{n}{i}\PY{p}{]}
        
                \PY{c+c1}{\PYZsh{}get account age and membership details}
                \PY{n}{age\PYZus{}membership} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//a[@href=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZsh{}super\PYZhy{}secret\PYZhy{}}
        \PY{n}{activity}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/div/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()}
        
                \PY{c+c1}{\PYZsh{}sometimes the age is wrapped up in a span}
                \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{age\PYZus{}membership}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                    \PY{n}{age} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//a[@href=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZsh{}super\PYZhy{}secret\PYZhy{}}
        \PY{n}{activity}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/span/div/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()[0]}
                    \PY{n}{age\PYZus{}membership}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{age}\PY{p}{)}
                \PY{n}{image}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{account\PYZus{}age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{age\PYZus{}membership}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        
        
                \PY{c+c1}{\PYZsh{}get artist personal details}
                \PY{n}{artist\PYZus{}details} \PY{o}{=} \PY{n}{response}\PY{o}{.}\PY{n}{xpath}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{//div[@id=}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{super\PYZhy{}secret\PYZhy{}}
        \PY{n}{why}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{]/div/div/div/dl/dd/text()}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{).extract()}
                \PY{n}{details} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}asl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{artist\PYZus{}dob}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{c+c1}{\PYZsh{}some artists do not share their dobs}
                \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{artist\PYZus{}details}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{image}\PY{p}{[}\PY{n}{details}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{artist\PYZus{}details}\PY{p}{[}\PY{n}{i}\PY{p}{]}
        
                \PY{k}{return} \PY{n}{image}
        
        \PY{n}{process} \PY{o}{=} \PY{n}{CrawlerProcess}\PY{p}{(}\PY{p}{)}
        \PY{n}{process}\PY{o}{.}\PY{n}{crawl}\PY{p}{(}\PY{n}{ImageSpider}\PY{p}{)}
        \PY{n}{process}\PY{o}{.}\PY{n}{start}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\section{Appendix: Analysis Code}
\label{appendix:code}
\url{https://nbviewer.jupyter.org/github/hueyning/DA-scraper/blob/master/python-notebook/DA-analysis.ipynb}
    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
